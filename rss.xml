<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0"><channel><title>Data Everythingist</title><link>http://blairhudson.github.io/blog/</link><description>The personal blog of Blair Hudson, The Data Everythingist</description><atom:link rel="self" type="application/rss+xml" href="http://blairhudson.github.io/blog/rss.xml"></atom:link><language>en</language><copyright>Contents © 2017 &lt;a href="http://blairhudson.com"&gt;Blair Hudson&lt;/a&gt; </copyright><lastBuildDate>Wed, 13 Sep 2017 11:13:08 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Optimising hyper-parameters efficiently with Scikit-Optimize</title><link>http://blairhudson.github.io/blog/posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/</link><dc:creator>Blair Hudson</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;One of the most well-known techniques for experimenting with various model configurations is &lt;em&gt;Grid Search&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;With grid search, you specify a discrete search space (a parameter grid) of all of the parameter values you would like to test. The search permutes through the grid, testing various combinations until all are exhausted. Basic a specified performance metric (e.g. error), you can select the best parameter combination for your model.&lt;/p&gt;
&lt;p&gt;What's wrong with this?&lt;/p&gt;
&lt;p&gt;If you have a large parameter grid, this doesn't work too well:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;param_grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;'param_a'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'param_b'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'param_c'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;num_searches&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param_grid&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prod&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;param_grid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;
    
&lt;span class="n"&gt;num_searches&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param_grid&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[1]:&lt;/div&gt;



&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;27&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;And maybe we want to search over four possible values instead for &lt;code&gt;param_a&lt;/code&gt;, and add two more new parameters:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [2]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;param_grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;'param_a'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.03&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'param_b'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'param_c'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'param_d'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"a"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"b"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'param_e'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;num_searches&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param_grid&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[2]:&lt;/div&gt;



&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;216&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;As you can see from the first grid, there's already 27 combinations to try. Then this jumps to 216 for our larger grid. Depending on the complexity of the model and the amount of data to process, this can very easily become infeasible.&lt;/p&gt;
&lt;p&gt;There are a few approaches to solving this, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;breaking down the search into multiple smaller steps (such as searching &lt;code&gt;param_a&lt;/code&gt; and &lt;code&gt;param_b&lt;/code&gt; first, with defaults for the others, then using the best values to search the remaining parameters - this can be tricky in practice)&lt;/li&gt;
&lt;li&gt;searching the parameter space &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"&gt;at random&lt;/a&gt; (which has an additional benefit of discovering better parameter values when random samples are drawn frmo a continuous range)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While Scikit-Learn doesn't provide many more options, some &lt;a href="https://github.com/scikit-optimize/scikit-optimize/blob/master/AUTHORS.md"&gt;clever people&lt;/a&gt; have developed a drop-in replacement for Scikit-Learn's &lt;code&gt;GridSearchCV&lt;/code&gt; and &lt;code&gt;RandomizedSearchCV&lt;/code&gt; called &lt;code&gt;BayesSearchCV&lt;/code&gt; in a package called &lt;em&gt;Scikit-Optimize&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let's install Scikit-Optimize and implement &lt;code&gt;BayesSearchCV&lt;/code&gt; with a simple example!&lt;/p&gt;
&lt;h3 id="Installing-Scikit-Optimize"&gt;Installing Scikit-Optimize&lt;a class="anchor-link" href="http://blairhudson.github.io/blog/posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/#Installing-Scikit-Optimize"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Assuming you already have already &lt;a href="http://blairhudson.github.io/blog/posts/using-jupyter-notebooks-with-anaconda/"&gt;installed Anaconda and Jupyter&lt;/a&gt;, you will need to do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pip install scikit-optimize&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you have trouble installing, you may first need to run the following to install one of Scikit-Optmize's dependencies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pip install scikit-garden&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="Implementing-BayesSearchCV"&gt;Implementing &lt;code&gt;BayesSearchCV&lt;/code&gt;&lt;a class="anchor-link" href="http://blairhudson.github.io/blog/posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/#Implementing-BayesSearchCV"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Here's an example implementation using a sample dataset and &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"&gt;Logistic Regression&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;skopt&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BayesSearchCV&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_breast_cancer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;

&lt;span class="c1"&gt;# prep some sample data&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_breast_cancer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;return_X_y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# we're using a logistic regression model&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# this is our parameter grid&lt;/span&gt;
&lt;span class="n"&gt;param_grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;'solver'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'liblinear'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'saga'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  
    &lt;span class="s1"&gt;'penalty'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'l1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;'l2'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s1"&gt;'tol'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'log-uniform'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'log-uniform'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;'fit_intercept'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# set up our optimiser to find the best params in 30 searches&lt;/span&gt;
&lt;span class="n"&gt;opt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BayesSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;param_grid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;opt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.
  FutureWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=0.000382254605256, C=0.000825787721576, solver=saga, penalty=l2 
[CV]  fit_intercept=True, tol=0.000382254605256, C=0.000825787721576, solver=saga, penalty=l2, total=   0.1s
[CV] fit_intercept=True, tol=0.000382254605256, C=0.000825787721576, solver=saga, penalty=l2 
[CV]  fit_intercept=True, tol=0.000382254605256, C=0.000825787721576, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=0.000382254605256, C=0.000825787721576, solver=saga, penalty=l2 
[CV]  fit_intercept=True, tol=0.000382254605256, C=0.000825787721576, solver=saga, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=0.000555351500381, C=0.243756153601, solver=saga, penalty=l2 
[CV]  fit_intercept=True, tol=0.000555351500381, C=0.243756153601, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=0.000555351500381, C=0.243756153601, solver=saga, penalty=l2 
[CV]  fit_intercept=True, tol=0.000555351500381, C=0.243756153601, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=0.000555351500381, C=0.243756153601, solver=saga, penalty=l2 
[CV]  fit_intercept=True, tol=0.000555351500381, C=0.243756153601, solver=saga, penalty=l2, total=   0.0s
Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=0.00021856668705, C=0.000379398151643, solver=liblinear, penalty=l1 
[CV]  fit_intercept=True, tol=0.00021856668705, C=0.000379398151643, solver=liblinear, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=0.00021856668705, C=0.000379398151643, solver=liblinear, penalty=l1 
[CV]  fit_intercept=True, tol=0.00021856668705, C=0.000379398151643, solver=liblinear, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=0.00021856668705, C=0.000379398151643, solver=liblinear, penalty=l1 
[CV]  fit_intercept=True, tol=0.00021856668705, C=0.000379398151643, solver=liblinear, penalty=l1, total=   0.0s
Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=0.000102890478113, C=0.00840588302113, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=0.000102890478113, C=0.00840588302113, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=0.000102890478113, C=0.00840588302113, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=0.000102890478113, C=0.00840588302113, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=0.000102890478113, C=0.00840588302113, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=0.000102890478113, C=0.00840588302113, solver=liblinear, penalty=l2, total=   0.0s
Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=0.000920403419315, C=0.00194382983221, solver=saga, penalty=l1 
[CV]  fit_intercept=True, tol=0.000920403419315, C=0.00194382983221, solver=saga, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=0.000920403419315, C=0.00194382983221, solver=saga, penalty=l1 
[CV]  fit_intercept=True, tol=0.000920403419315, C=0.00194382983221, solver=saga, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=0.000920403419315, C=0.00194382983221, solver=saga, penalty=l1 
[CV]  fit_intercept=True, tol=0.000920403419315, C=0.00194382983221, solver=saga, penalty=l1, total=   0.0s
Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=3.94082520634e-05, C=0.000130052012491, solver=liblinear, penalty=l1 
[CV]  fit_intercept=True, tol=3.94082520634e-05, C=0.000130052012491, solver=liblinear, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=3.94082520634e-05, C=0.000130052012491, solver=liblinear, penalty=l1 
[CV]  fit_intercept=True, tol=3.94082520634e-05, C=0.000130052012491, solver=liblinear, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=3.94082520634e-05, C=0.000130052012491, solver=liblinear, penalty=l1 
[CV]  fit_intercept=True, tol=3.94082520634e-05, C=0.000130052012491, solver=liblinear, penalty=l1, total=   0.0s
Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=0.000349866443736, C=0.704155417474, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=0.000349866443736, C=0.704155417474, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.000349866443736, C=0.704155417474, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=0.000349866443736, C=0.704155417474, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.000349866443736, C=0.704155417474, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=0.000349866443736, C=0.704155417474, solver=saga, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=0.000550604891749, C=1.95909841601, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.000550604891749, C=1.95909841601, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.000550604891749, C=1.95909841601, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.000550604891749, C=1.95909841601, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.000550604891749, C=1.95909841601, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.000550604891749, C=1.95909841601, solver=liblinear, penalty=l2, total=   0.0s
Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=2.57731741777e-05, C=0.0933134709093, solver=saga, penalty=l1 
[CV]  fit_intercept=True, tol=2.57731741777e-05, C=0.0933134709093, solver=saga, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=2.57731741777e-05, C=0.0933134709093, solver=saga, penalty=l1 
[CV]  fit_intercept=True, tol=2.57731741777e-05, C=0.0933134709093, solver=saga, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=2.57731741777e-05, C=0.0933134709093, solver=saga, penalty=l1 
[CV]  fit_intercept=True, tol=2.57731741777e-05, C=0.0933134709093, solver=saga, penalty=l1, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=5.06278923847e-05, C=5.86409384188, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=5.06278923847e-05, C=5.86409384188, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=5.06278923847e-05, C=5.86409384188, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=5.06278923847e-05, C=5.86409384188, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=5.06278923847e-05, C=5.86409384188, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=5.06278923847e-05, C=5.86409384188, solver=saga, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=0.001, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.001, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.001, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.001, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.001, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.001, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=0.001, C=0.00171621716873, solver=liblinear, penalty=l1 
[CV]  fit_intercept=True, tol=0.001, C=0.00171621716873, solver=liblinear, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=0.001, C=0.00171621716873, solver=liblinear, penalty=l1 
[CV]  fit_intercept=True, tol=0.001, C=0.00171621716873, solver=liblinear, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=0.001, C=0.00171621716873, solver=liblinear, penalty=l1 
[CV]  fit_intercept=True, tol=0.001, C=0.00171621716873, solver=liblinear, penalty=l1, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=0.000742149491069, C=4.65040156797, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.000742149491069, C=4.65040156797, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.000742149491069, C=4.65040156797, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.000742149491069, C=4.65040156797, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.000742149491069, C=4.65040156797, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.000742149491069, C=4.65040156797, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=0.000289050608323, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.000289050608323, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.000289050608323, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.000289050608323, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.000289050608323, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.000289050608323, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=1e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=1e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=1e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=1e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=1e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=1e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=0.000157629057632, C=100.0, solver=saga, penalty=l1 
[CV]  fit_intercept=False, tol=0.000157629057632, C=100.0, solver=saga, penalty=l1, total=   0.0s
[CV] fit_intercept=False, tol=0.000157629057632, C=100.0, solver=saga, penalty=l1 
[CV]  fit_intercept=False, tol=0.000157629057632, C=100.0, solver=saga, penalty=l1, total=   0.0s
[CV] fit_intercept=False, tol=0.000157629057632, C=100.0, solver=saga, penalty=l1 
[CV]  fit_intercept=False, tol=0.000157629057632, C=100.0, solver=saga, penalty=l1, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=1e-05, C=1e-05, solver=saga, penalty=l2 .
[CV]  fit_intercept=True, tol=1e-05, C=1e-05, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1e-05, C=1e-05, solver=saga, penalty=l2 .
[CV]  fit_intercept=True, tol=1e-05, C=1e-05, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1e-05, C=1e-05, solver=saga, penalty=l2 .
[CV]  fit_intercept=True, tol=1e-05, C=1e-05, solver=saga, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=1e-05, C=100.0, solver=saga, penalty=l1 
[CV]  fit_intercept=False, tol=1e-05, C=100.0, solver=saga, penalty=l1, total=   0.0s
[CV] fit_intercept=False, tol=1e-05, C=100.0, solver=saga, penalty=l1 
[CV]  fit_intercept=False, tol=1e-05, C=100.0, solver=saga, penalty=l1, total=   0.0s
[CV] fit_intercept=False, tol=1e-05, C=100.0, solver=saga, penalty=l1 
[CV]  fit_intercept=False, tol=1e-05, C=100.0, solver=saga, penalty=l1, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=9.55256066036e-05, C=0.0267622531962, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=9.55256066036e-05, C=0.0267622531962, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=9.55256066036e-05, C=0.0267622531962, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=9.55256066036e-05, C=0.0267622531962, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=9.55256066036e-05, C=0.0267622531962, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=9.55256066036e-05, C=0.0267622531962, solver=saga, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=2.14150718478e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=2.14150718478e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=2.14150718478e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=2.14150718478e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=2.14150718478e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=2.14150718478e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=1e-05, C=38.4658080596, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=38.4658080596, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1e-05, C=38.4658080596, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=38.4658080596, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1e-05, C=38.4658080596, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=38.4658080596, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=0.001, C=3.18302458993, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.001, C=3.18302458993, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.001, C=3.18302458993, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.001, C=3.18302458993, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.001, C=3.18302458993, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=0.001, C=3.18302458993, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=1e-05, C=61.9366266622, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=1e-05, C=61.9366266622, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=1e-05, C=61.9366266622, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=1e-05, C=61.9366266622, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=1e-05, C=61.9366266622, solver=liblinear, penalty=l2 
[CV]  fit_intercept=False, tol=1e-05, C=61.9366266622, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=1.52191042725e-05, C=1e-05, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1.52191042725e-05, C=1e-05, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1.52191042725e-05, C=1e-05, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1.52191042725e-05, C=1e-05, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1.52191042725e-05, C=1e-05, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1.52191042725e-05, C=1e-05, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=5.84228443388e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=5.84228443388e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=5.84228443388e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=5.84228443388e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=5.84228443388e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=5.84228443388e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=False, tol=0.00054754356556, C=0.00366650548222, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=0.00054754356556, C=0.00366650548222, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.00054754356556, C=0.00366650548222, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=0.00054754356556, C=0.00366650548222, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=False, tol=0.00054754356556, C=0.00366650548222, solver=saga, penalty=l2 
[CV]  fit_intercept=False, tol=0.00054754356556, C=0.00366650548222, solver=saga, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=5.54807277677e-05, C=0.825090969286, solver=saga, penalty=l1 
[CV]  fit_intercept=True, tol=5.54807277677e-05, C=0.825090969286, solver=saga, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=5.54807277677e-05, C=0.825090969286, solver=saga, penalty=l1 
[CV]  fit_intercept=True, tol=5.54807277677e-05, C=0.825090969286, solver=saga, penalty=l1, total=   0.0s
[CV] fit_intercept=True, tol=5.54807277677e-05, C=0.825090969286, solver=saga, penalty=l1 
[CV]  fit_intercept=True, tol=5.54807277677e-05, C=0.825090969286, solver=saga, penalty=l1, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
/Users/blairhudson/anaconda/lib/python3.5/site-packages/skopt/optimizer/optimizer.py:366: UserWarning: The objective has been evaluated at this point before.
  warnings.warn("The objective has been evaluated "
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=100.0, solver=liblinear, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Fitting 3 folds for each of 1 candidates, totalling 3 fits
[CV] fit_intercept=True, tol=1e-05, C=63.2457136517, solver=saga, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=63.2457136517, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1e-05, C=63.2457136517, solver=saga, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=63.2457136517, solver=saga, penalty=l2, total=   0.0s
[CV] fit_intercept=True, tol=1e-05, C=63.2457136517, solver=saga, penalty=l2 
[CV]  fit_intercept=True, tol=1e-05, C=63.2457136517, solver=saga, penalty=l2, total=   0.0s
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stderr output_text"&gt;
&lt;pre&gt;/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
/Users/blairhudson/anaconda/lib/python3.5/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
  "the coef_ did not converge", ConvergenceWarning)
[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [4]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Best params achieve a test score of'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;opt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;':'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;opt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;

&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Best params achieve a test score of 0.951048951049 :
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[4]:&lt;/div&gt;



&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;{'C': 100.0,
 'fit_intercept': True,
 'penalty': 'l2',
 'solver': 'liblinear',
 'tol': 1.0000000000000001e-05}&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;By increasing the value of &lt;code&gt;n_iter&lt;/code&gt;, you can continue the search to find better parameter combinations. You can also use the optimiser for prediction, by calling &lt;code&gt;.predict()&lt;/code&gt; or &lt;code&gt;.predict_proba()&lt;/code&gt; for probabilities, or extract and use the best one standalone:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [5]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;opt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_estimator_&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[5]:&lt;/div&gt;



&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=1234, solver='liblinear',
          tol=1.0000000000000001e-05, verbose=0, warm_start=False)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;You may also find it useful to re-use the best parameters programatically to define an equivalent model:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [6]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;opt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[6]:&lt;/div&gt;



&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear',
          tol=1.0000000000000001e-05, verbose=0, warm_start=False)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
</description><guid>http://blairhudson.github.io/blog/posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/</guid><pubDate>Wed, 13 Sep 2017 11:12:52 GMT</pubDate></item><item><title>Using Jupyter notebooks with Anaconda</title><link>http://blairhudson.github.io/blog/posts/using-jupyter-notebooks-with-anaconda/</link><dc:creator>Blair Hudson</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Jupyter is a popular data science environment, and Jupyter notebooks (such as &lt;a href="http://blairhudson.github.io/blog/posts/using-jupyter-notebooks-with-anaconda/index.ipynb"&gt;the notebook this post&lt;/a&gt; was written with) are a great way to create and share great data science with inline documentation (using Markdown syntax).&lt;/p&gt;
&lt;p&gt;Jupyter is capable of running kernels in many different programming languages, but in this post we're focussed just on Python.&lt;/p&gt;
&lt;h3 id="Installing-Anaconda"&gt;Installing Anaconda&lt;a class="anchor-link" href="http://blairhudson.github.io/blog/posts/using-jupyter-notebooks-with-anaconda/#Installing-Anaconda"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Download Anaconda for your operating system from the &lt;a href="https://www.anaconda.com/download"&gt;Anaconda website&lt;/a&gt;. For best compatibility with modern data science packages, I suggest Python 3.6 version or newer.&lt;/li&gt;
&lt;li&gt;Run the downloaded installer and follow the prompts.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="Launching-Jupyter"&gt;Launching Jupyter&lt;a class="anchor-link" href="http://blairhudson.github.io/blog/posts/using-jupyter-notebooks-with-anaconda/#Launching-Jupyter"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Run the following command to launch the Jupyter environment in your current directory:
 &lt;code&gt;jupyter notebook&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;By default, this will open the web interface in your default web browser, and by default at &lt;a href="http://localhost:8888/"&gt;http://localhost:8888/&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Now you can select an existing &lt;code&gt;.ipynb&lt;/code&gt; file from the file navigator to open it, or create a new notebook.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;🚀&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
</description><guid>http://blairhudson.github.io/blog/posts/using-jupyter-notebooks-with-anaconda/</guid><pubDate>Tue, 12 Sep 2017 11:20:54 GMT</pubDate></item><item><title>Creating a blog with Jupyter notebooks</title><link>http://blairhudson.github.io/blog/posts/creating-a-blog-with-jupyter-notebooks/</link><dc:creator>Blair Hudson</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Assuming you already have already &lt;a href="http://blairhudson.github.io/blog/posts/using-jupyter-notebooks-with-anaconda/"&gt;installed Jupyter notebook&lt;/a&gt;, you will need to do the following:&lt;/p&gt;
&lt;h3 id="Installing-and-configuring-a-Nikola-blog"&gt;Installing and configuring a Nikola blog&lt;a class="anchor-link" href="http://blairhudson.github.io/blog/posts/creating-a-blog-with-jupyter-notebooks/#Installing-and-configuring-a-Nikola-blog"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;First you'll need to create a directory structure as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; - /blog
 -- /posts
 -- /output&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/blog&lt;/code&gt; is the root directory for everything you'll be doing with your blog&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/posts&lt;/code&gt; is where you'll store your Jupyter notebooks&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/output&lt;/code&gt; will contain the code generated for your blog&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the following command to install Nikola (the static website generator which will do most of the heavy lifting)&lt;sup&gt;[1]&lt;/sup&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip install --upgrade "Nikola[extras]"&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Change directory to your blog root:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cd blog&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Start up Nikola, following the prompts to configure your new blog:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;nikola init .&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open &lt;code&gt;/blog/conf.py&lt;/code&gt; and change the &lt;code&gt;POSTS&lt;/code&gt; and &lt;code&gt;PAGES&lt;/code&gt; sections to include the lines as follows. This will allow Nikola to treat &lt;code&gt;.ipynb&lt;/code&gt; files as blog posts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; POSTS = (
     ("posts/*.rst", "posts", "post.tmpl"),
     ("posts/*.md", "posts", "post.tmpl"),
     ("posts/*.txt", "posts", "post.tmpl"),
     ("posts/*.html", "posts", "post.tmpl"),
     ("posts/*.ipynb", "posts", "post.tmpl"),
 )
 PAGES = (
     ("pages/*.rst", "pages", "page.tmpl"),
     ("pages/*.md", "pages", "page.tmpl"),
     ("pages/*.txt", "pages", "page.tmpl"),
     ("pages/*.html", "pages", "page.tmpl"),
     ("pages/*.ipynb", "pages", "page.tmpl"),
 )&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write your blog post in Jupyter, saving the &lt;code&gt;.ipynb&lt;/code&gt; file to &lt;code&gt;/posts&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You will need to explicitly add the following metadata to your notebook (in the Jupyter menu, select &lt;em&gt;Edit &amp;gt; Edit Notebook Metadata&lt;/em&gt;). Change the metadata to match your post.&lt;sup&gt;[2]&lt;/sup&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; "nikola": {
     "title": "Creating a blog with Jupyter notebooks",
     "slug": "creating-a-blog-with-jupyter-notebooks",
     "date": "2017-09-09 21:09:01 UTC+10:00"
 }&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;code&gt;nikola build&lt;/code&gt; each time you update your &lt;code&gt;/posts&lt;/code&gt;, which will generate your site and store it in &lt;code&gt;/output&lt;/code&gt;!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you're going to be publishing your blog on Github (like me), you can push the content of &lt;code&gt;/output&lt;/code&gt; to your website repo (&lt;a href="https://github.com/blairhudson/blog"&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="[1]Problems-installing-Nikola?"&gt;&lt;sup&gt;[1]&lt;/sup&gt;Problems installing Nikola?&lt;a class="anchor-link" href="http://blairhudson.github.io/blog/posts/creating-a-blog-with-jupyter-notebooks/#%5B1%5DProblems-installing-Nikola?"&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;I ran into some issues installing Nikola on OS X with Anaconda. Specifically, &lt;code&gt;gcc&lt;/code&gt; in Anaconda was the culprit. Resolution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;conda remove gcc&lt;/code&gt; to uninstall &lt;code&gt;gcc&lt;/code&gt; provided by Anaconda&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will default to the system &lt;code&gt;gcc&lt;/code&gt;, which you can check by running &lt;code&gt;which gcc&lt;/code&gt; (which should output &lt;code&gt;/usr/bin/gcc&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If this still doesn't resolve the issue still, you may need to install a more up-to-date &lt;code&gt;gcc&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="https://brew.sh"&gt;Homebrew&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;brew install gcc&lt;/code&gt; (you may be prompted to install Developer Tools)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;brew unlink gcc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;brew link --overwrite gcc&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;which gcc&lt;/code&gt; should now show &lt;code&gt;/usr/local/Cellar/gcc/7.2.0&lt;/code&gt;. 👍&lt;/p&gt;
&lt;h4 id="[2]Inferring-Nikola-post-metadata"&gt;&lt;sup&gt;[2]&lt;/sup&gt;Inferring Nikola post metadata&lt;a class="anchor-link" href="http://blairhudson.github.io/blog/posts/creating-a-blog-with-jupyter-notebooks/#%5B2%5DInferring-Nikola-post-metadata"&gt;¶&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Like me, you probably want as little as possible to come between your latest notebook hack and your awesome new blog.&lt;/p&gt;
&lt;p&gt;Nikola parses Jupyter notebooks with a plugin, which with some modification we can have infer all of the Nikola post metadata automatically. For me, the plugin file was here (though it may differ for you):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;~/anaconda/lib/python3.5/site-packages/nikola/plugins/compile/ipynb.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To automagically infer the required metadata, you can replace the &lt;code&gt;read_metadata()&lt;/code&gt; function in the file above with the following code:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [5]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_metadata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file_metadata_regexp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;unslugify_titles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Read metadata directly from ipynb file.&lt;/span&gt;

&lt;span class="sd"&gt;    As ipynb file support arbitrary metadata as json, the metadata used by Nikola&lt;/span&gt;
&lt;span class="sd"&gt;    will be assume to be in the 'nikola' subfield.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_req_missing_ipynb&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;lang&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;lang&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LocaleBorg&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;current_lang&lt;/span&gt;
    &lt;span class="n"&gt;source&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;translated_source_path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"r"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"utf8"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;in_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;nb_json&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nbformat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;in_file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;current_nbformat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# Metadata might not exist in two-file posts or in hand-crafted&lt;/span&gt;
    &lt;span class="c1"&gt;# .ipynb files.&lt;/span&gt;

    &lt;span class="c1"&gt;# infer metadata&lt;/span&gt;
    &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="p"&gt;))[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;slug&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;' '&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'-'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;
    &lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromtimestamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getctime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strftime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'%Y-%m-&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; %k:%M:%S'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;implicit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'title'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'slug'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;slug&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'date'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;explicit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nb_json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'metadata'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'nikola'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;
    
    &lt;span class="c1"&gt;# replace inference with explicit if available&lt;/span&gt;
    &lt;span class="n"&gt;metadata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;implicit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;explicit&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;metadata&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;With this small modification, we instruct Nikola to infer the &lt;code&gt;title&lt;/code&gt; and &lt;code&gt;slug&lt;/code&gt; values based on the filename, and the &lt;code&gt;date&lt;/code&gt; value based on the filesystem. Magical! ✨&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
</description><guid>http://blairhudson.github.io/blog/posts/creating-a-blog-with-jupyter-notebooks/</guid><pubDate>Tue, 12 Sep 2017 11:00:45 GMT</pubDate></item></channel></rss>