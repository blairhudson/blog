{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most well-known techniques for experimenting with various model configurations is _Grid Search_.\n",
    "\n",
    "With grid search, you specify a discrete search space (a parameter grid) of all of the parameter values you would like to test. The search permutes through the grid, testing various combinations until all are exhausted. Basic a specified performance metric (e.g. error), you can select the best parameter combination for your model.\n",
    "\n",
    "What's wrong with this?\n",
    "\n",
    "If you have a large parameter grid, this doesn't work too well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "param_grid = {\n",
    "    'param_a': [0.01, 0.03, 0.1],\n",
    "    'param_b': [0, 1, 2],\n",
    "    'param_c': [1, 50, 100]\n",
    "}\n",
    "\n",
    "def num_searches(param_grid):\n",
    "    return np.prod([len(p) for p in param_grid.values()])\n",
    "    \n",
    "num_searches(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And maybe we want to search over four possible values instead for `param_a`, and add two more new parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'param_a': [0.01, 0.03, 0.1, 0.3],\n",
    "    'param_b': [0, 1, 2],\n",
    "    'param_c': [1, 50, 100],\n",
    "    'param_d': [\"a\", \"b\"],\n",
    "    'param_e': [0, 1, 2]\n",
    "}\n",
    "\n",
    "num_searches(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the first grid, there's already 27 combinations to try. Then this jumps to 216 for our larger grid. Depending on the complexity of the model and the amount of data to process, this can very easily become infeasible.\n",
    "\n",
    "There are a few approaches to solving this, including:\n",
    "\n",
    "* breaking down the search into multiple smaller steps (such as searching `param_a` and `param_b` first, with defaults for the others, then using the best values to search the remaining parameters - this can be tricky in practice)\n",
    "* searching the parameter space [at random](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) (which has an additional benefit of discovering better parameter values when random samples are drawn frmo a continuous range)\n",
    "\n",
    "While Scikit-Learn doesn't provide many more options, some [clever people](https://github.com/scikit-optimize/scikit-optimize/blob/master/AUTHORS.md) have developed a drop-in replacement for Scikit-Learn's `GridSearchCV` and `RandomizedSearchCV` called `BayesSearchCV` in a package called *Scikit-Optimize*.\n",
    "\n",
    "Let's install Scikit-Optimize and implement `BayesSearchCV` with a simple example!\n",
    "\n",
    "## Installing Scikit-Optimize\n",
    "\n",
    "Assuming you already have already [installed Anaconda and Jupyter](/posts/using-jupyter-notebooks-with-anaconda/), you will need to do the following:\n",
    "\n",
    "* `pip install scikit-optimize`\n",
    "\n",
    "If you have trouble installing, you may first need to run the following to install one of Scikit-Optmize's dependencies:\n",
    "\n",
    "* `pip install scikit-garden`\n",
    "\n",
    "## Implementing BayesSearchCV\n",
    "\n",
    "Here's an example implementation using a sample dataset and [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# prep some sample data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=1234)\n",
    "\n",
    "# we're using a logistic regression model\n",
    "clf = LogisticRegression(random_state=1234, verbose=0)\n",
    "\n",
    "# this is our parameter grid\n",
    "param_grid = {\n",
    "    'solver': ['liblinear', 'saga'],  \n",
    "    'penalty': ['l1','l2'],\n",
    "    'tol': (1e-5, 1e-3, 'log-uniform'),\n",
    "    'C': (1e-5, 100, 'log-uniform'),\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# set up our optimiser to find the best params in 30 searches\n",
    "opt = BayesSearchCV(\n",
    "    clf,\n",
    "    param_grid,\n",
    "    n_iter=30,\n",
    "    random_state=1234,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params achieve a test score of 0.958041958042 :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 100.0,\n",
       " 'fit_intercept': True,\n",
       " 'penalty': 'l1',\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.00094035472283658726}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Best params achieve a test score of', opt.score(X_test, y_test), ':')\n",
    "\n",
    "opt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the value of `n_iter`, you can continue the search to find better parameter combinations. You can also use the optimiser for prediction, by calling `.predict()` or `.predict_proba()` for probabilities, or extract and use the best one standalone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=1234, solver='liblinear',\n",
       "          tol=0.00094035472283658726, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also find it useful to re-use the best parameters programatically to define an equivalent model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear',\n",
       "          tol=0.00094035472283658726, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(**opt.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
