{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous post we learned how to [access a notebook programmatically](/posts/accessing-jupyter-notebooks-programatically) using the `ipynb` package.\n",
    "\n",
    "This is very powerful as it allows a data scientist to focus on implementing a model which is re-usable, specifying a `fit` and `predict` method to provide some structure to their code.\n",
    "\n",
    "In this post, we're going to build a utility wrapper which takes the previous code and the following functionality:\n",
    "\n",
    "* Serialization, so we don't have to re-fit models if we don't need to\n",
    "* Scoring, so we can determine how well our model is performing\n",
    "* Feature importance, so we can determine the predictive power of individual features - and provide insight into feature selection\n",
    "\n",
    "## Building the wrapper\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import os.path\n",
    "\n",
    "class ModelUtils(object):\n",
    "    \n",
    "    def __init__(self, model, serialize_path=None):\n",
    "        \"\"\"\n",
    "            If serialize_path is specified and valid, load the model from disk.\n",
    "        \"\"\"\n",
    "        self.serialize_path = serialize_path\n",
    "        \n",
    "        if self.serialize_path is not None and os.path.exists(self.serialize_path):\n",
    "            print('Loaded from', self.serialize_path)\n",
    "            self.clf = joblib.load(self.serialize_path)\n",
    "            self.is_fitted = True\n",
    "            return\n",
    "        \n",
    "        self.clf = model\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "            Fit our model, saving the model to disk if serialize_path is specified.\n",
    "        \"\"\"\n",
    "        # fit our model\n",
    "        self.clf.fit(X, y)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # serialise to path\n",
    "        if self.serialize_path is not None:\n",
    "            joblib.dump(self.clf, self.serialize_path)\n",
    "            print('Saved to', self.serialize_path)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.is_fitted:\n",
    "            raise NotFittedError\n",
    "        return self.clf.predict(X)\n",
    "    \n",
    "    def score(self, X, y_true):\n",
    "        \"\"\"\n",
    "            Generates a score for the model based on predicting on X and comparing \n",
    "            to y_true.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "    def feature_importance(self, X, y, normalize=True):\n",
    "        \"\"\"\n",
    "            To calculate feature importance, we iterate through each feature i, \n",
    "            generating a model score with all other features zeroed.\n",
    "            \n",
    "            If normalize is True, divide the results by the minimum score, such that\n",
    "            each score represents \"N times better than the worst feature\".\n",
    "        \"\"\"\n",
    "        scores = [self.score(self.__zero_except(X, i), y) for i in range(X.shape[1])]\n",
    "        \n",
    "        if normalize:\n",
    "            return scores / min(scores)\n",
    "        return scores\n",
    "    \n",
    "    def __zero_except(self, X, i):\n",
    "        \"\"\"\n",
    "            A helper function to replace all but the ith column with zeroes, and \n",
    "            return the result. (There is probably a cleaner way to do this.)\n",
    "        \"\"\"\n",
    "        X_copy = X.copy()\n",
    "        X_i = X[:,i]\n",
    "        X_copy.fill(0)\n",
    "        X_copy[:,i] = X_i\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the wrapper\n",
    "\n",
    "Now we have our `ModelUtils` wrapper class, lets import `CustomModel` [as before](/posts/accessing-jupyter-notebooks-programatically) and put it to work.\n",
    "\n",
    "As we instantiate the wrapper, we're specifying `test.pkl` in the current directory as the location to serialize the model.\n",
    "\n",
    "If `serialize_path` is configured and valid, the pre-fitted model will be loaded from there, and the `predict` function will be immediately available. If configured but the file does not exist, `ModelUtils` will serialize to this location after fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.model import CustomModel\n",
    "\n",
    "model = ModelUtils(CustomModel(), serialize_path='test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up the sample data again, and fit our model and then use it to create some predictions. Note the `Saved to test.pkl` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to test.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    train_size=0.75, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=1234) # more reproducibility\n",
    "\n",
    "# fit our model (as before)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# generate some predictions (as before)\n",
    "model.predict(X_test);  # fun fact: the ; character suppresses notebook output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and feature performance\n",
    "\n",
    "Finally, here are our two new functions.\n",
    "\n",
    "First, let's score the performance of our model. This is using a metric called ROC AUC - we won't explain what that is in this post in any detail, but essentially it is a measure of how well the model can separate each class in `y`.\n",
    "\n",
    "Then we will calculate relative feature importance for each of the 30 features in the sample dataset. Based on individual scoring performance, what this means is the the first feature is ~2.12x more powerful than the lowest performing feature, and the best feature is ~35.86x more powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.989669421488\n",
      "Top feature relative performance: 35.855513308\n",
      "[  2.121673     8.6730038   34.85551331   2.06844106  25.14068441\n",
      "  29.74904943  33.68060837  34.68441065  24.88973384  15.51711027\n",
      "   4.66920152  18.64638783   4.85931559  34.7148289   16.36121673\n",
      "  24.82129278  27.2851711   27.96958175  15.74904943  16.24334601   1.\n",
      "  29.3269962   35.85551331  35.83269962  26.1634981   30.85931559\n",
      "  33.77186312  35.12927757  27.24714829  23.27376426]\n"
     ]
    }
   ],
   "source": [
    "# score our model\n",
    "auc = model.score(X_test, y_test)\n",
    "print('AUC:', auc)\n",
    "\n",
    "# calculate relative feature importance\n",
    "importance = model.feature_importance(X_test, y_test)\n",
    "\n",
    "print('Top feature relative performance:', max(importance))\n",
    "print(model.feature_importance(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëè"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
