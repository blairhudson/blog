<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="The personal blog of Blair Hudson, The Data Everythingist">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Data Everythingist (page 1) | Data Everythingist</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="http://blairhudson.github.io/blog/">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/using-bivariate-kernel-density-estimation-for-plotting-multi-task-classification-results/" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md navbar-dark bg-dark static-top mb-4"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="http://blairhudson.github.io/blog/">

            <span id="blog-title">Data Everythingist</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="rss.xml" class="nav-link">RSS</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        

    


    
<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/using-bivariate-kernel-density-estimation-for-plotting-multi-task-classification-results/" class="u-url">Using bivariate kernel density estimation for plotting multi-task classification results</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Blair Hudson
            </span></p>
            <p class="dateline"><a href="posts/using-bivariate-kernel-density-estimation-for-plotting-multi-task-classification-results/" rel="bookmark"><time class="published dt-published" datetime="2017-11-19T22:32:39+11:00" title="2017-11-19 22:32">2017-11-19 22:32</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One common technique for interpreting the outputs of a single classification model is kernel density estimation (KDE). Similar to a histogram, a KDE plot allows us to estimate the underlying probability density of our model.</p>
<p>This is particularly useful for visualising the impact of selecting different classification thresholds (i.e. deciding at what point to round a given probability to 1 or 0).</p>
<p>You can apply this visualisation technique to multi-task classifcation too. This uses bivariate KDE, which also generalises to multivariate KDE. Unforunately we're constrained to two tasks, given the limitation of having only two axes on 2D plots.</p>
<p>To achieve this, we're going to create a suitable test dataset based on the Digits classification data, train a Random Forest Classifier using two labels, and output a bivariate KDE plot using the Seaborn visualisation library.</p>
<h3 id="Preparing-data-for-multi-task-learning">Preparing data for multi-task learning<a class="anchor-link" href="posts/using-bivariate-kernel-density-estimation-for-plotting-multi-task-classification-results/#Preparing-data-for-multi-task-learning">¶</a>
</h3>
<p>To simulate multi-task learning, we're going to load three classes of the Digits data (i.e. digits 0, 1 and 2), and break this into labels for two binary tasks:</p>
<ol>
<li>the digit is one</li>
<li>the digit is two</li>
</ol>
<p>(In the case that the digit is zero, both tasks will have a False label.)</p>
<p>Before training our model, we'll also consolidate the two label sets into a single 2 x N set.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_digits</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_class</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">y_task1</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">1</span>
<span class="n">y_task2</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">2</span>

<span class="n">y_multitask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">y_task1</span><span class="p">,</span><span class="n">y_task2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-a-multi-task-Random-Forest-classifier">Training a multi-task Random Forest classifier<a class="anchor-link" href="posts/using-bivariate-kernel-density-estimation-for-plotting-multi-task-classification-results/#Training-a-multi-task-Random-Forest-classifier">¶</a>
</h3>
<p>After splitting our data into train and test sets, we'll train the classifier and split the predicted probabilities into two sets:</p>
<ol>
<li>probability that the digit is one</li>
<li>probability that the digit is two</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_multitask</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_task1_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y_task2_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Visualisation-bivariate-KDE-with-Seaborn">Visualisation bivariate KDE with Seaborn<a class="anchor-link" href="posts/using-bivariate-kernel-density-estimation-for-plotting-multi-task-classification-results/#Visualisation-bivariate-KDE-with-Seaborn">¶</a>
</h3>
<p>Seaborn (a set of extensions over Matplotlib) comes to the rescue with the built-in KDE plot function. The function accepts two data sets, one for the X-axis (in this case, task 1) and one fo the Y-axis (task 2).</p>
<p>We've also constrained the axis to the range (0,1). Without this, the full range of the KDE function will be plotted, going below 0% and above 100%, which doesn't make sense in the context of our problem (or any binary classification task).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">kdeplot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">y_task1_hat</span><span class="p">,</span> <span class="n">y_task2_hat</span><span class="p">,</span> <span class="n">shade</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'dotted'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'grey'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'dotted'</span><span class="p">)</span>
<span class="n">kdeplot</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">'P(Task 1)'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">'P(Task 2)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYcAAAEJCAYAAAB/pOvWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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">
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The bivariate KDE plot makes the probability densities for our two learning tasks clear, and as you would expect, demonstrates that it is not likely for a given case to have a high probability for both tasks simulatenously.</p>

</div>
</div>
</div>
    </div>
  </div>

    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/automating-error-analysis-with-rulefit-models/" class="u-url">Automating error analysis with RuleFit models</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Blair Hudson
            </span></p>
            <p class="dateline"><a href="posts/automating-error-analysis-with-rulefit-models/" rel="bookmark"><time class="published dt-published" datetime="2017-09-27T23:01:51+10:00" title="2017-09-27 23:01">2017-09-27 23:01</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When building machine learning models, the goal is generally to improve the performance of a model based on some performance metric. One of the most simple metrics is <em>error</em>. Error is simply the inverse of model accuracy - so if a model had 95% accuracy, this would correspond with 5% error.</p>
<p>There are many ways to improve the performance of a model and subsequently decrease the model error. This includes adding more training observations (rows), enriching training obversations with more features (columns), modifying the model algorithm or <a href="posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/">optimising the algorithm parameters</a>.</p>
<h3 id="Reducing-error-by-introducing-new-features">Reducing error by introducing new features<a class="anchor-link" href="posts/automating-error-analysis-with-rulefit-models/#Reducing-error-by-introducing-new-features">¶</a>
</h3>
<p>In the post linked above we looked at how to optimise the parameters of a given algorithm, so for now we're interested in what we can do with the data itself.</p>
<p>While there are many ways to create more training observations, this is often infeasible due to consideration of cost (imagine the cost of high-end medical studies) and time (such as waiting for enough events to occur).</p>
<p>The next option we have is to introduce new features to the observations already in our model. There is often lots of different approaches here too, including:</p>
<ul>
<li>engineering new features based on existing features</li>
<li>creating new features from available data not already used</li>
<li>making more data available (such as from external providers)</li>
</ul>
<p>The goal of this post is to explore a method not for assessing which approach to take, but for identifying where the gaps are to help you assess all of the options available to you.</p>
<h3 id="Modelling-error-analysis">Modelling error analysis<a class="anchor-link" href="posts/automating-error-analysis-with-rulefit-models/#Modelling-error-analysis">¶</a>
</h3>
<p>To get started, let's install an implementation of RuleFit from GitHub using pip:</p>

<pre><code>pip install git+https://github.com/christophM/rulefit</code></pre>
<p>Now we're going to load up a sample data set to work on, partitioning it into data for training our initial model, and data for testing its performance. Note that feature names will be important for this exercise.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="c1"># load our data - we also care about feature names</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">data</span><span class="o">.</span><span class="n">feature_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">' '</span> <span class="p">,</span> <span class="s1">'_'</span><span class="p">),</span> <span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">))</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># split data for training and testing</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> 
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span> <span class="c1"># more reproducibility</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With our data ready, let's build a quick logistic regression model on the training data. We're also going to generate predictions for our test data (as positive probabilities, or the likelihood of the class label being True).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># define our model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>

<span class="c1"># fit our model</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># generate some predictions</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At the start of this post we discussed model error, so let's now calculate this for our model to see how much room for improvement there is.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># calculate error on each obversation in the test set</span>
<span class="n">y_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span>

<span class="c1"># is there much room for improvement?</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'model error:'</span><span class="p">,</span> <span class="n">y_error</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>model error: 0.0705335674785
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It looks like there is almost 7% <em>mean absolute error</em>. Maybe we can find some good leads for improving on this?</p>
<p>To do so, we're going to create a new model using the <code>RuleFit</code> class, but instead of targetting the original class label <em>y</em>, we're going to calculate the <em>absolute error</em> of each observation.</p>
<p>The absolute error is the difference between the discrete actual value of <em>y</em> (0 or 1) and the continuous positive probability we predicted (0.0 to 1.0).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">rulefit</span> <span class="k">import</span> <span class="n">RuleFit</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">GradientBoostingRegressor</span>

<span class="c1"># define and fit our shiny new RuleFit model</span>
<span class="n">generator_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>       <span class="c1"># control the complexity of our rules</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>  
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.003</span><span class="p">,</span>
    <span class="s1">'random_state'</span><span class="p">:</span> <span class="mi">1234</span>
<span class="p">}</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">generator_params</span><span class="p">)</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RuleFit</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_error</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[4]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>RuleFit(tree_generator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.003, loss='ls', max_depth=5,
             max_features=None, max_leaf_nodes=None,
             min_impurity_decrease=0.0, min_impurity_split=None,
             min_samples_leaf=1, min_samples_split=2,
             min_weight_fraction_leaf=0.0, n_estimators=1000,
             presort='auto', random_state=1234, subsample=1.0, verbose=0,
             warm_start=False))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With the <code>RuleFit</code> model fitted to our errors, we can generate a set of rules that might help us to isolate areas of our data that need enriching with new features.</p>
<p><code>RuleFit</code> actually generates rules for the data having a positive impact on the model, but we can ignore these for error analysis for filtering <code>coef</code> &gt; 0.</p>
<p>If we multiply the coefficient and support values calculated by <code>RuleFit</code>, we can use that as a rough estimate for how much error is due to that subset of the data.</p>
<p>By summing these estimates, we get an approximate amount of error explained by these rules. This will differ from the above simply because our rules may not perfect fit our errors (that is, our error model has its own error).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># get the outputs</span>
<span class="n">rules</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">get_rules</span><span class="p">()</span>

<span class="c1"># remove the rules we're not interested in. if the coefficient isn't above 0</span>
<span class="c1"># there rule is not a good indicator of an area for improvement</span>
<span class="n">rules</span> <span class="o">=</span> <span class="n">rules</span><span class="p">[(</span><span class="n">rules</span><span class="o">.</span><span class="n">coef</span> <span class="o">&gt;</span> <span class="mf">0.</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">rules</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s1">'linear'</span><span class="p">)]</span>

<span class="c1"># we can estimate an effect for each rule on the error score from above by </span>
<span class="c1"># multiplying the coefficient and support values</span>
<span class="n">rules</span><span class="p">[</span><span class="s1">'effect'</span><span class="p">]</span> <span class="o">=</span> <span class="n">rules</span><span class="p">[</span><span class="s1">'coef'</span><span class="p">]</span> <span class="o">*</span> <span class="n">rules</span><span class="p">[</span><span class="s1">'support'</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'modelled error:'</span><span class="p">,</span> <span class="n">rules</span><span class="p">[</span><span class="s1">'effect'</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'unexplained error:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_error</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">rules</span><span class="p">[</span><span class="s1">'effect'</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span><span class="mi">0</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>modelled error: 0.046811390404179753
unexplained error: 0.0237221770743
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's take a look at the top 10 rules:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># display the top 10 rules by effect</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">'display.max_colwidth'</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rules</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="s1">'effect'</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[6]:</div>


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
<thead><tr style="text-align: right;">
<th></th>
      <th>rule</th>
      <th>type</th>
      <th>coef</th>
      <th>support</th>
      <th>effect</th>
    </tr></thead>
<tbody>
<tr>
<th>883</th>
      <td>worst_area &lt;= 976.25 &amp; worst_area &gt; 553.299987793 &amp; radius_error &gt; 0.275350004435</td>
      <td>rule</td>
      <td>0.056166</td>
      <td>0.216783</td>
      <td>0.012176</td>
    </tr>
<tr>
<th>1176</th>
      <td>compactness_error &gt; 0.0203649997711 &amp; worst_fractal_dimension &lt;= 0.113150000572 &amp; area_error &gt; 23.2399997711 &amp; radius_error &lt;= 0.339100003242</td>
      <td>rule</td>
      <td>0.265549</td>
      <td>0.041958</td>
      <td>0.011142</td>
    </tr>
<tr>
<th>769</th>
      <td>worst_area &gt; 548.650024414 &amp; area_error &gt; 23.2350006104 &amp; worst_area &lt;= 976.25</td>
      <td>rule</td>
      <td>0.028937</td>
      <td>0.223776</td>
      <td>0.006475</td>
    </tr>
<tr>
<th>458</th>
      <td>worst_radius &gt; 15.345000267 &amp; worst_concavity &gt; 0.207249999046 &amp; worst_symmetry &gt; 0.203749999404 &amp; worst_symmetry &lt;= 0.560700058937 &amp; worst_radius &lt;= 16.8100013733</td>
      <td>rule</td>
      <td>0.101535</td>
      <td>0.041958</td>
      <td>0.004260</td>
    </tr>
<tr>
<th>1548</th>
      <td>worst_symmetry &gt; 0.203749999404 &amp; area_error &lt;= 33.375 &amp; worst_symmetry &lt;= 0.560700058937 &amp; worst_perimeter &gt; 100.555000305</td>
      <td>rule</td>
      <td>0.024403</td>
      <td>0.153846</td>
      <td>0.003754</td>
    </tr>
<tr>
<th>5361</th>
      <td>radius_error &gt; 0.344399988651 &amp; symmetry_error &lt;= 0.017725000158 &amp; area_error &gt; 23.2399997711 &amp; worst_perimeter &lt;= 105.199996948</td>
      <td>rule</td>
      <td>0.133092</td>
      <td>0.027972</td>
      <td>0.003723</td>
    </tr>
<tr>
<th>357</th>
      <td>worst_radius &gt; 15.5699996948 &amp; worst_symmetry &gt; 0.203749999404 &amp; mean_texture &gt; 19.1049995422 &amp; worst_symmetry &lt;= 0.560700058937 &amp; radius_error &lt;= 0.418200016022</td>
      <td>rule</td>
      <td>0.032694</td>
      <td>0.069930</td>
      <td>0.002286</td>
    </tr>
<tr>
<th>57</th>
      <td>worst_area &gt; 548.650024414 &amp; worst_area &lt;= 1086.0</td>
      <td>rule</td>
      <td>0.002326</td>
      <td>0.405594</td>
      <td>0.000943</td>
    </tr>
<tr>
<th>1433</th>
      <td>worst_area &gt; 548.650024414 &amp; mean_perimeter &gt; 79.2050018311 &amp; mean_texture &gt; 21.1399993896 &amp; area_error &lt;= 36.4049987793</td>
      <td>rule</td>
      <td>0.012087</td>
      <td>0.069930</td>
      <td>0.000845</td>
    </tr>
<tr>
<th>5547</th>
      <td>worst_area &gt; 744.0 &amp; worst_symmetry &gt; 0.203749999404 &amp; mean_texture &gt; 19.1049995422 &amp; worst_symmetry &lt;= 0.560700058937 &amp; radius_error &lt;= 0.418200016022</td>
      <td>rule</td>
      <td>0.004637</td>
      <td>0.069930</td>
      <td>0.000324</td>
    </tr>
</tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To wrap up, let's produce a report of the top 3 rules, including up to 10 examples from the data to which the rules apply.</p>
<p>This report can be used in conjunction with subject-matter experitise on the data to isolate areas for feature enrichment, to improve your model!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.core.display</span> <span class="k">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># prepare a dataframe for use below (we really care about the `query` function)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'y_error'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_error</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'y_sq_error'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_error</span><span class="o">**</span><span class="mi">2</span>

<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">rule</span> <span class="ow">in</span> <span class="n">rules</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s1">'effect'</span><span class="p">)</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'rule:'</span><span class="p">,</span> <span class="n">rule</span><span class="p">[</span><span class="s1">'rule'</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'support:'</span><span class="p">,</span> <span class="n">rule</span><span class="p">[</span><span class="s1">'support'</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'coef:'</span><span class="p">,</span> <span class="n">rule</span><span class="p">[</span><span class="s1">'coef'</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'estimated error effect (support x coef):'</span><span class="p">,</span> <span class="n">rule</span><span class="p">[</span><span class="s1">'effect'</span><span class="p">])</span>
    
    <span class="c1"># it might be useful to compare the local error to the estimated model effect</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'rule MAE:'</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">rule</span><span class="p">[</span><span class="s1">'rule'</span><span class="p">])[</span><span class="s1">'y_error'</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'rule RMSE:'</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">rule</span><span class="p">[</span><span class="s1">'rule'</span><span class="p">])[</span><span class="s1">'y_sq_error'</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
    
    <span class="c1"># we can use the rule to filter the data</span>
    <span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">rule</span><span class="p">[</span><span class="s1">'rule'</span><span class="p">])</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="s1">'y_error'</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>rule: worst_area &lt;= 976.25 &amp; worst_area &gt; 553.299987793 &amp; radius_error &gt; 0.275350004435
support: 0.21678321678321677
coef: 0.05616597023836435
estimated error effect (support x coef): 0.012175839702023041
rule MAE: 0.2658110370900938
rule RMSE: 0.4155759177730745
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
<thead><tr style="text-align: right;">
<th></th>
      <th>mean_radius</th>
      <th>mean_texture</th>
      <th>mean_perimeter</th>
      <th>mean_area</th>
      <th>mean_smoothness</th>
      <th>mean_compactness</th>
      <th>mean_concavity</th>
      <th>mean_concave_points</th>
      <th>mean_symmetry</th>
      <th>mean_fractal_dimension</th>
      <th>...</th>
      <th>worst_perimeter</th>
      <th>worst_area</th>
      <th>worst_smoothness</th>
      <th>worst_compactness</th>
      <th>worst_concavity</th>
      <th>worst_concave_points</th>
      <th>worst_symmetry</th>
      <th>worst_fractal_dimension</th>
      <th>y_error</th>
      <th>y_sq_error</th>
    </tr></thead>
<tbody>
<tr>
<th>91</th>
      <td>11.76</td>
      <td>18.14</td>
      <td>75.00</td>
      <td>431.1</td>
      <td>0.09968</td>
      <td>0.05914</td>
      <td>0.02685</td>
      <td>0.03515</td>
      <td>0.1619</td>
      <td>0.06287</td>
      <td>...</td>
      <td>85.10</td>
      <td>553.6</td>
      <td>0.1137</td>
      <td>0.07974</td>
      <td>0.0612</td>
      <td>0.07160</td>
      <td>0.1978</td>
      <td>0.06915</td>
      <td>0.974325</td>
      <td>0.949308</td>
    </tr>
<tr>
<th>83</th>
      <td>15.37</td>
      <td>22.76</td>
      <td>100.20</td>
      <td>728.2</td>
      <td>0.09200</td>
      <td>0.10360</td>
      <td>0.11220</td>
      <td>0.07483</td>
      <td>0.1717</td>
      <td>0.06097</td>
      <td>...</td>
      <td>107.50</td>
      <td>830.9</td>
      <td>0.1257</td>
      <td>0.19970</td>
      <td>0.2846</td>
      <td>0.14760</td>
      <td>0.2556</td>
      <td>0.06828</td>
      <td>0.929329</td>
      <td>0.863652</td>
    </tr>
<tr>
<th>89</th>
      <td>14.60</td>
      <td>23.29</td>
      <td>93.97</td>
      <td>664.7</td>
      <td>0.08682</td>
      <td>0.06636</td>
      <td>0.08390</td>
      <td>0.05271</td>
      <td>0.1627</td>
      <td>0.05416</td>
      <td>...</td>
      <td>102.20</td>
      <td>758.2</td>
      <td>0.1312</td>
      <td>0.15810</td>
      <td>0.2675</td>
      <td>0.13590</td>
      <td>0.2477</td>
      <td>0.06836</td>
      <td>0.856296</td>
      <td>0.733242</td>
    </tr>
<tr>
<th>47</th>
      <td>13.80</td>
      <td>15.79</td>
      <td>90.43</td>
      <td>584.1</td>
      <td>0.10070</td>
      <td>0.12800</td>
      <td>0.07789</td>
      <td>0.05069</td>
      <td>0.1662</td>
      <td>0.06566</td>
      <td>...</td>
      <td>110.30</td>
      <td>812.4</td>
      <td>0.1411</td>
      <td>0.35420</td>
      <td>0.2779</td>
      <td>0.13830</td>
      <td>0.2589</td>
      <td>0.10300</td>
      <td>0.832785</td>
      <td>0.693531</td>
    </tr>
<tr>
<th>137</th>
      <td>14.22</td>
      <td>27.85</td>
      <td>92.55</td>
      <td>623.9</td>
      <td>0.08223</td>
      <td>0.10390</td>
      <td>0.11030</td>
      <td>0.04408</td>
      <td>0.1342</td>
      <td>0.06129</td>
      <td>...</td>
      <td>102.50</td>
      <td>764.0</td>
      <td>0.1081</td>
      <td>0.24260</td>
      <td>0.3064</td>
      <td>0.08219</td>
      <td>0.1890</td>
      <td>0.07796</td>
      <td>0.707592</td>
      <td>0.500686</td>
    </tr>
<tr>
<th>73</th>
      <td>11.80</td>
      <td>16.58</td>
      <td>78.99</td>
      <td>432.0</td>
      <td>0.10910</td>
      <td>0.17000</td>
      <td>0.16590</td>
      <td>0.07415</td>
      <td>0.2678</td>
      <td>0.07371</td>
      <td>...</td>
      <td>91.93</td>
      <td>591.7</td>
      <td>0.1385</td>
      <td>0.40920</td>
      <td>0.4504</td>
      <td>0.18650</td>
      <td>0.5774</td>
      <td>0.10300</td>
      <td>0.703641</td>
      <td>0.495111</td>
    </tr>
<tr>
<th>130</th>
      <td>14.99</td>
      <td>22.11</td>
      <td>97.53</td>
      <td>693.7</td>
      <td>0.08515</td>
      <td>0.10250</td>
      <td>0.06859</td>
      <td>0.03876</td>
      <td>0.1944</td>
      <td>0.05913</td>
      <td>...</td>
      <td>110.20</td>
      <td>867.1</td>
      <td>0.1077</td>
      <td>0.33450</td>
      <td>0.3114</td>
      <td>0.13080</td>
      <td>0.3163</td>
      <td>0.09251</td>
      <td>0.646361</td>
      <td>0.417782</td>
    </tr>
<tr>
<th>111</th>
      <td>13.27</td>
      <td>14.76</td>
      <td>84.74</td>
      <td>551.7</td>
      <td>0.07355</td>
      <td>0.05055</td>
      <td>0.03261</td>
      <td>0.02648</td>
      <td>0.1386</td>
      <td>0.05318</td>
      <td>...</td>
      <td>104.50</td>
      <td>830.6</td>
      <td>0.1006</td>
      <td>0.12380</td>
      <td>0.1350</td>
      <td>0.10010</td>
      <td>0.2027</td>
      <td>0.06206</td>
      <td>0.471311</td>
      <td>0.222134</td>
    </tr>
<tr>
<th>119</th>
      <td>16.25</td>
      <td>19.51</td>
      <td>109.80</td>
      <td>815.8</td>
      <td>0.10260</td>
      <td>0.18930</td>
      <td>0.22360</td>
      <td>0.09194</td>
      <td>0.2151</td>
      <td>0.06578</td>
      <td>...</td>
      <td>122.10</td>
      <td>939.7</td>
      <td>0.1377</td>
      <td>0.44620</td>
      <td>0.5897</td>
      <td>0.17750</td>
      <td>0.3318</td>
      <td>0.09136</td>
      <td>0.467160</td>
      <td>0.218238</td>
    </tr>
<tr>
<th>25</th>
      <td>13.90</td>
      <td>19.24</td>
      <td>88.73</td>
      <td>602.9</td>
      <td>0.07991</td>
      <td>0.05326</td>
      <td>0.02995</td>
      <td>0.02070</td>
      <td>0.1579</td>
      <td>0.05594</td>
      <td>...</td>
      <td>104.40</td>
      <td>830.5</td>
      <td>0.1064</td>
      <td>0.14150</td>
      <td>0.1673</td>
      <td>0.08150</td>
      <td>0.2356</td>
      <td>0.07603</td>
      <td>0.344350</td>
      <td>0.118577</td>
    </tr>
</tbody>
</table>
<p>10 rows × 32 columns</p>
</div>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>rule: compactness_error &gt; 0.0203649997711 &amp; worst_fractal_dimension &lt;= 0.113150000572 &amp; area_error &gt; 23.2399997711 &amp; radius_error &lt;= 0.339100003242
support: 0.04195804195804196
coef: 0.26554881554597404
estimated error effect (support x coef): 0.011141908344586324
rule MAE: 0.7144778689742627
rule RMSE: 0.7290404836131931
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
<thead><tr style="text-align: right;">
<th></th>
      <th>mean_radius</th>
      <th>mean_texture</th>
      <th>mean_perimeter</th>
      <th>mean_area</th>
      <th>mean_smoothness</th>
      <th>mean_compactness</th>
      <th>mean_concavity</th>
      <th>mean_concave_points</th>
      <th>mean_symmetry</th>
      <th>mean_fractal_dimension</th>
      <th>...</th>
      <th>worst_perimeter</th>
      <th>worst_area</th>
      <th>worst_smoothness</th>
      <th>worst_compactness</th>
      <th>worst_concavity</th>
      <th>worst_concave_points</th>
      <th>worst_symmetry</th>
      <th>worst_fractal_dimension</th>
      <th>y_error</th>
      <th>y_sq_error</th>
    </tr></thead>
<tbody>
<tr>
<th>83</th>
      <td>15.37</td>
      <td>22.76</td>
      <td>100.20</td>
      <td>728.2</td>
      <td>0.09200</td>
      <td>0.1036</td>
      <td>0.11220</td>
      <td>0.07483</td>
      <td>0.1717</td>
      <td>0.06097</td>
      <td>...</td>
      <td>107.50</td>
      <td>830.9</td>
      <td>0.1257</td>
      <td>0.1997</td>
      <td>0.2846</td>
      <td>0.14760</td>
      <td>0.2556</td>
      <td>0.06828</td>
      <td>0.929329</td>
      <td>0.863652</td>
    </tr>
<tr>
<th>47</th>
      <td>13.80</td>
      <td>15.79</td>
      <td>90.43</td>
      <td>584.1</td>
      <td>0.10070</td>
      <td>0.1280</td>
      <td>0.07789</td>
      <td>0.05069</td>
      <td>0.1662</td>
      <td>0.06566</td>
      <td>...</td>
      <td>110.30</td>
      <td>812.4</td>
      <td>0.1411</td>
      <td>0.3542</td>
      <td>0.2779</td>
      <td>0.13830</td>
      <td>0.2589</td>
      <td>0.10300</td>
      <td>0.832785</td>
      <td>0.693531</td>
    </tr>
<tr>
<th>137</th>
      <td>14.22</td>
      <td>27.85</td>
      <td>92.55</td>
      <td>623.9</td>
      <td>0.08223</td>
      <td>0.1039</td>
      <td>0.11030</td>
      <td>0.04408</td>
      <td>0.1342</td>
      <td>0.06129</td>
      <td>...</td>
      <td>102.50</td>
      <td>764.0</td>
      <td>0.1081</td>
      <td>0.2426</td>
      <td>0.3064</td>
      <td>0.08219</td>
      <td>0.1890</td>
      <td>0.07796</td>
      <td>0.707592</td>
      <td>0.500686</td>
    </tr>
<tr>
<th>73</th>
      <td>11.80</td>
      <td>16.58</td>
      <td>78.99</td>
      <td>432.0</td>
      <td>0.10910</td>
      <td>0.1700</td>
      <td>0.16590</td>
      <td>0.07415</td>
      <td>0.2678</td>
      <td>0.07371</td>
      <td>...</td>
      <td>91.93</td>
      <td>591.7</td>
      <td>0.1385</td>
      <td>0.4092</td>
      <td>0.4504</td>
      <td>0.18650</td>
      <td>0.5774</td>
      <td>0.10300</td>
      <td>0.703641</td>
      <td>0.495111</td>
    </tr>
<tr>
<th>130</th>
      <td>14.99</td>
      <td>22.11</td>
      <td>97.53</td>
      <td>693.7</td>
      <td>0.08515</td>
      <td>0.1025</td>
      <td>0.06859</td>
      <td>0.03876</td>
      <td>0.1944</td>
      <td>0.05913</td>
      <td>...</td>
      <td>110.20</td>
      <td>867.1</td>
      <td>0.1077</td>
      <td>0.3345</td>
      <td>0.3114</td>
      <td>0.13080</td>
      <td>0.3163</td>
      <td>0.09251</td>
      <td>0.646361</td>
      <td>0.417782</td>
    </tr>
<tr>
<th>119</th>
      <td>16.25</td>
      <td>19.51</td>
      <td>109.80</td>
      <td>815.8</td>
      <td>0.10260</td>
      <td>0.1893</td>
      <td>0.22360</td>
      <td>0.09194</td>
      <td>0.2151</td>
      <td>0.06578</td>
      <td>...</td>
      <td>122.10</td>
      <td>939.7</td>
      <td>0.1377</td>
      <td>0.4462</td>
      <td>0.5897</td>
      <td>0.17750</td>
      <td>0.3318</td>
      <td>0.09136</td>
      <td>0.467160</td>
      <td>0.218238</td>
    </tr>
</tbody>
</table>
<p>6 rows × 32 columns</p>
</div>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>rule: worst_area &gt; 548.650024414 &amp; area_error &gt; 23.2350006104 &amp; worst_area &lt;= 976.25
support: 0.22377622377622378
coef: 0.02893728555291459
estimated error effect (support x coef): 0.006475476487365502
rule MAE: 0.2578116453233828
rule RMSE: 0.40903437023906347
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
<thead><tr style="text-align: right;">
<th></th>
      <th>mean_radius</th>
      <th>mean_texture</th>
      <th>mean_perimeter</th>
      <th>mean_area</th>
      <th>mean_smoothness</th>
      <th>mean_compactness</th>
      <th>mean_concavity</th>
      <th>mean_concave_points</th>
      <th>mean_symmetry</th>
      <th>mean_fractal_dimension</th>
      <th>...</th>
      <th>worst_perimeter</th>
      <th>worst_area</th>
      <th>worst_smoothness</th>
      <th>worst_compactness</th>
      <th>worst_concavity</th>
      <th>worst_concave_points</th>
      <th>worst_symmetry</th>
      <th>worst_fractal_dimension</th>
      <th>y_error</th>
      <th>y_sq_error</th>
    </tr></thead>
<tbody>
<tr>
<th>91</th>
      <td>11.76</td>
      <td>18.14</td>
      <td>75.00</td>
      <td>431.1</td>
      <td>0.09968</td>
      <td>0.05914</td>
      <td>0.02685</td>
      <td>0.03515</td>
      <td>0.1619</td>
      <td>0.06287</td>
      <td>...</td>
      <td>85.10</td>
      <td>553.6</td>
      <td>0.1137</td>
      <td>0.07974</td>
      <td>0.0612</td>
      <td>0.07160</td>
      <td>0.1978</td>
      <td>0.06915</td>
      <td>0.974325</td>
      <td>0.949308</td>
    </tr>
<tr>
<th>83</th>
      <td>15.37</td>
      <td>22.76</td>
      <td>100.20</td>
      <td>728.2</td>
      <td>0.09200</td>
      <td>0.10360</td>
      <td>0.11220</td>
      <td>0.07483</td>
      <td>0.1717</td>
      <td>0.06097</td>
      <td>...</td>
      <td>107.50</td>
      <td>830.9</td>
      <td>0.1257</td>
      <td>0.19970</td>
      <td>0.2846</td>
      <td>0.14760</td>
      <td>0.2556</td>
      <td>0.06828</td>
      <td>0.929329</td>
      <td>0.863652</td>
    </tr>
<tr>
<th>89</th>
      <td>14.60</td>
      <td>23.29</td>
      <td>93.97</td>
      <td>664.7</td>
      <td>0.08682</td>
      <td>0.06636</td>
      <td>0.08390</td>
      <td>0.05271</td>
      <td>0.1627</td>
      <td>0.05416</td>
      <td>...</td>
      <td>102.20</td>
      <td>758.2</td>
      <td>0.1312</td>
      <td>0.15810</td>
      <td>0.2675</td>
      <td>0.13590</td>
      <td>0.2477</td>
      <td>0.06836</td>
      <td>0.856296</td>
      <td>0.733242</td>
    </tr>
<tr>
<th>47</th>
      <td>13.80</td>
      <td>15.79</td>
      <td>90.43</td>
      <td>584.1</td>
      <td>0.10070</td>
      <td>0.12800</td>
      <td>0.07789</td>
      <td>0.05069</td>
      <td>0.1662</td>
      <td>0.06566</td>
      <td>...</td>
      <td>110.30</td>
      <td>812.4</td>
      <td>0.1411</td>
      <td>0.35420</td>
      <td>0.2779</td>
      <td>0.13830</td>
      <td>0.2589</td>
      <td>0.10300</td>
      <td>0.832785</td>
      <td>0.693531</td>
    </tr>
<tr>
<th>137</th>
      <td>14.22</td>
      <td>27.85</td>
      <td>92.55</td>
      <td>623.9</td>
      <td>0.08223</td>
      <td>0.10390</td>
      <td>0.11030</td>
      <td>0.04408</td>
      <td>0.1342</td>
      <td>0.06129</td>
      <td>...</td>
      <td>102.50</td>
      <td>764.0</td>
      <td>0.1081</td>
      <td>0.24260</td>
      <td>0.3064</td>
      <td>0.08219</td>
      <td>0.1890</td>
      <td>0.07796</td>
      <td>0.707592</td>
      <td>0.500686</td>
    </tr>
<tr>
<th>73</th>
      <td>11.80</td>
      <td>16.58</td>
      <td>78.99</td>
      <td>432.0</td>
      <td>0.10910</td>
      <td>0.17000</td>
      <td>0.16590</td>
      <td>0.07415</td>
      <td>0.2678</td>
      <td>0.07371</td>
      <td>...</td>
      <td>91.93</td>
      <td>591.7</td>
      <td>0.1385</td>
      <td>0.40920</td>
      <td>0.4504</td>
      <td>0.18650</td>
      <td>0.5774</td>
      <td>0.10300</td>
      <td>0.703641</td>
      <td>0.495111</td>
    </tr>
<tr>
<th>130</th>
      <td>14.99</td>
      <td>22.11</td>
      <td>97.53</td>
      <td>693.7</td>
      <td>0.08515</td>
      <td>0.10250</td>
      <td>0.06859</td>
      <td>0.03876</td>
      <td>0.1944</td>
      <td>0.05913</td>
      <td>...</td>
      <td>110.20</td>
      <td>867.1</td>
      <td>0.1077</td>
      <td>0.33450</td>
      <td>0.3114</td>
      <td>0.13080</td>
      <td>0.3163</td>
      <td>0.09251</td>
      <td>0.646361</td>
      <td>0.417782</td>
    </tr>
<tr>
<th>111</th>
      <td>13.27</td>
      <td>14.76</td>
      <td>84.74</td>
      <td>551.7</td>
      <td>0.07355</td>
      <td>0.05055</td>
      <td>0.03261</td>
      <td>0.02648</td>
      <td>0.1386</td>
      <td>0.05318</td>
      <td>...</td>
      <td>104.50</td>
      <td>830.6</td>
      <td>0.1006</td>
      <td>0.12380</td>
      <td>0.1350</td>
      <td>0.10010</td>
      <td>0.2027</td>
      <td>0.06206</td>
      <td>0.471311</td>
      <td>0.222134</td>
    </tr>
<tr>
<th>119</th>
      <td>16.25</td>
      <td>19.51</td>
      <td>109.80</td>
      <td>815.8</td>
      <td>0.10260</td>
      <td>0.18930</td>
      <td>0.22360</td>
      <td>0.09194</td>
      <td>0.2151</td>
      <td>0.06578</td>
      <td>...</td>
      <td>122.10</td>
      <td>939.7</td>
      <td>0.1377</td>
      <td>0.44620</td>
      <td>0.5897</td>
      <td>0.17750</td>
      <td>0.3318</td>
      <td>0.09136</td>
      <td>0.467160</td>
      <td>0.218238</td>
    </tr>
<tr>
<th>25</th>
      <td>13.90</td>
      <td>19.24</td>
      <td>88.73</td>
      <td>602.9</td>
      <td>0.07991</td>
      <td>0.05326</td>
      <td>0.02995</td>
      <td>0.02070</td>
      <td>0.1579</td>
      <td>0.05594</td>
      <td>...</td>
      <td>104.40</td>
      <td>830.5</td>
      <td>0.1064</td>
      <td>0.14150</td>
      <td>0.1673</td>
      <td>0.08150</td>
      <td>0.2356</td>
      <td>0.07603</td>
      <td>0.344350</td>
      <td>0.118577</td>
    </tr>
</tbody>
</table>
<p>10 rows × 32 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>✨</p>

</div>
</div>
</div>
    </div>
  </div>

    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/introduction-to-classification-using-logistic-regression-with-scikit-learn/" class="u-url">Introduction to Classification using Logistic Regression with Scikit-Learn</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Blair Hudson
            </span></p>
            <p class="dateline"><a href="posts/introduction-to-classification-using-logistic-regression-with-scikit-learn/" rel="bookmark"><time class="published dt-published" datetime="2017-09-20T21:09:58+10:00" title="2017-09-20 21:09">2017-09-20 21:09</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This post, including the source <code>.ipynb</code> notebook file, will be used as a basis for other topics. You can obtain a copy of the source by clicking the <em>Source</em> link at the post of this post.</p>
<p>To keep things simple, we're going to utilise one of the <a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets">many toy datasets</a> built into Scikit-Learn! (And yes, it is a <a href="https://goo.gl/U2Uwz2">real dataset</a>.)</p>
<p>We're also not going to explain <em>how</em> Scikit-Learn's <code>LogisticRegression</code> is implemented in this post.</p>
<p>To structure our code, we will define our model in two parts:</p>
<ul>
<li>The code we need to fit our model</li>
<li>The code we need to use our fitted model to generate predictions</li>
</ul>
<p>When it comes to model building, these are the two main functional components - so, and for reasons which will be explained in other posts, we're going to build a Python class called <code>CustomModel</code>, with a function for each of these components:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>

<span class="k">class</span> <span class="nc">CustomModel</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

        <span class="c1"># LogisticRegression implements a number of parameters, you can read about them here:</span>
        <span class="c1"># http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</span>
        <span class="c1">#</span>
        <span class="c1"># With the exception of `random_state`, each of these are the defaults.</span>
        
        <span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">'penalty'</span><span class="p">:</span> <span class="s1">'l2'</span><span class="p">,</span>
            <span class="s1">'dual'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s1">'tol'</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>
            <span class="s1">'C'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
            <span class="s1">'fit_intercept'</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">'intercept_scaling'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s1">'class_weight'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">'random_state'</span><span class="p">:</span> <span class="mi">1234</span><span class="p">,</span>    <span class="c1"># Fixed to 1234 for reproducibility</span>
            <span class="s1">'solver'</span><span class="p">:</span> <span class="s1">'liblinear'</span><span class="p">,</span>
            <span class="s1">'max_iter'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
            <span class="s1">'multi_class'</span><span class="p">:</span> <span class="s1">'ovr'</span><span class="p">,</span>
            <span class="s1">'verbose'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s1">'warm_start'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s1">'n_jobs'</span><span class="p">:</span> <span class="mi">1</span>
        <span class="p">}</span>
    
        <span class="bp">self</span><span class="o">.</span><span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="o">**</span><span class="n">model_params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span> <span class="c1"># fun fact: returning self enables method chaining i.e. .fit().predict()</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        
        <span class="c1"># We only want to output the positive case (the second column returned by `predict_proba`:</span>
    
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we're ready to use our model!</p>
<p>In the next section we're going to load the sample data discussed above, and divide it into two portions:</p>
<ul>
<li>75% for model fitting</li>
<li>25% for predictions</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> 
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span> <span class="c1"># more reproducibility</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we have everything we need, lets load up our model, fit it with the training data, and generate some predictions:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># load our model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CustomModel</span><span class="p">()</span>

<span class="c1"># fit our model</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># generate some predictions</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[4]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>array([  9.25168417e-01,   9.99922130e-01,   9.53635418e-01,
         9.88416588e-01,   9.97542577e-01,   9.95232506e-01,
         4.60659258e-02,   9.98390194e-01,   6.59002902e-10,
         2.76899836e-06,   8.30718694e-10,   9.63993586e-01,
         9.94157890e-01,   9.50980576e-01,   9.96974859e-01,
         6.97038792e-10,   9.99809391e-01,   9.96431765e-01,
         9.99363563e-01,   8.43800531e-06,   9.95502414e-01,
         7.77576547e-03,   1.12727716e-09,   3.40904102e-17,
         3.68627970e-09,   6.55649762e-01,   3.51723839e-03,
         9.97326888e-01,   9.98785233e-01,   9.97552026e-01,
         9.86350517e-01,   9.98844211e-01,   5.70842717e-04,
         9.87742427e-01,   9.19814189e-01,   9.78443649e-01,
         9.92882821e-01,   1.14676290e-02,   1.48817234e-01,
         9.98733024e-01,   4.13813658e-05,   9.93177003e-01,
         1.72319657e-10,   8.54534408e-01,   8.81187668e-01,
         9.97568264e-01,   9.98086681e-01,   8.32784885e-01,
         4.49929586e-11,   8.89087737e-01,   9.28259947e-01,
         9.91244116e-01,   9.94876558e-01,   1.51106510e-08,
         2.60668778e-01,   9.99597520e-01,   9.98940073e-01,
         9.99968817e-01,   9.91318570e-01,   8.29369844e-03,
         9.93238377e-01,   9.92431535e-01,   9.29775117e-01,
         9.99271713e-01,   9.96474598e-01,   2.41572863e-04,
         1.51376226e-11,   9.97330558e-01,   9.98831771e-01,
         4.79400697e-01,   9.99798779e-01,   3.57307727e-07,
         9.99656809e-01,   7.03641088e-01,   9.98247027e-01,
         9.96093354e-01,   9.99588791e-01,   2.58369708e-08,
         9.98136922e-01,   7.97865310e-03,   9.99065333e-01,
         9.98470351e-01,   9.94581260e-01,   9.29328694e-01,
         1.41996390e-02,   1.43214384e-04,   3.71155631e-05,
         4.45838811e-06,   9.13207438e-01,   8.56295696e-01,
         9.99467328e-01,   9.74324559e-01,   9.99328632e-01,
         2.91312374e-12,   1.00998256e-01,   9.86992421e-01,
         9.97149193e-01,   9.13815924e-01,   9.98807818e-01,
         9.84005486e-01,   3.17865443e-08,   2.30937811e-11,
         9.98036358e-01,   9.99532884e-01,   1.24075526e-03,
         9.98819765e-01,   9.99752279e-01,   8.53677349e-04,
         1.53192255e-01,   9.30832406e-01,   1.49723823e-05,
         5.28688983e-01,   1.48786146e-03,   9.92804571e-51,
         8.86447353e-01,   9.95516043e-01,   9.98554149e-01,
         1.75078944e-03,   9.99922978e-01,   4.67159833e-01,
         9.99825913e-01,   9.57716419e-01,   9.95069689e-01,
         9.98728887e-01,   7.49375338e-14,   9.92513330e-01,
         1.49918676e-02,   1.63977226e-02,   9.95785292e-01,
         9.56124754e-01,   3.53639065e-01,   9.96011137e-01,
         7.27728677e-33,   9.97779030e-01,   7.77872222e-02,
         9.90058068e-01,   9.80367925e-01,   2.92408222e-01,
         9.98164180e-01,   1.67926421e-01,   9.99996297e-01,
         6.35631576e-10,   1.06440027e-01])</pre>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>

    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/building-a-utility-function-wrapper-for-scikit-learn-models/" class="u-url">Building a utility function wrapper for Scikit-Learn models</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Blair Hudson
            </span></p>
            <p class="dateline"><a href="posts/building-a-utility-function-wrapper-for-scikit-learn-models/" rel="bookmark"><time class="published dt-published" datetime="2017-09-17T21:04:40+10:00" title="2017-09-17 21:04">2017-09-17 21:04</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In a previous post we learned how to <a href="posts/accessing-jupyter-notebooks-programatically">access a notebook programmatically</a> using the <code>ipynb</code> package.</p>
<p>This is very powerful as it allows a data scientist to focus on implementing a model which is re-usable, specifying a <code>fit</code> and <code>predict</code> method to provide some structure to their code.</p>
<p>In this post, we're going to build a utility wrapper which takes the previous code and the following functionality:</p>
<ul>
<li>Serialization, so we don't have to re-fit models if we don't need to</li>
<li>Scoring, so we can determine how well our model is performing</li>
<li>Feature importance, so we can determine the predictive power of individual features - and provide insight into feature selection</li>
</ul>
<h3 id="Building-the-wrapper">Building the wrapper<a class="anchor-link" href="posts/building-a-utility-function-wrapper-for-scikit-learn-models/#Building-the-wrapper">¶</a>
</h3>
<p>Here is the code:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.externals</span> <span class="k">import</span> <span class="n">joblib</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">sklearn.exceptions</span> <span class="k">import</span> <span class="n">NotFittedError</span>
<span class="kn">import</span> <span class="nn">os.path</span>

<span class="k">class</span> <span class="nc">ModelUtils</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">serialize_path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">            If serialize_path is specified and valid, load the model from disk.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">serialize_path</span> <span class="o">=</span> <span class="n">serialize_path</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">serialize_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">serialize_path</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Loaded from'</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">serialize_path</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">clf</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">serialize_path</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">return</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">clf</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">            Fit our model, saving the model to disk if serialize_path is specified.</span>
<span class="sd">        """</span>
        <span class="c1"># fit our model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span> <span class="o">=</span> <span class="kc">True</span>
        
        <span class="c1"># serialise to path</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">serialize_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">serialize_path</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Saved to'</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">serialize_path</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">NotFittedError</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y_true</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">            Generates a score for the model based on predicting on X and comparing </span>
<span class="sd">            to y_true.</span>
<span class="sd">        """</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">feature_importance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">            To calculate feature importance, we iterate through each feature i, </span>
<span class="sd">            generating a model score with all other features zeroed.</span>
<span class="sd">            </span>
<span class="sd">            If normalize is True, divide the results by the minimum score, such that</span>
<span class="sd">            each score represents "N times better than the worst feature".</span>
<span class="sd">        """</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__zero_except</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
        
        <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span> <span class="o">/</span> <span class="nb">min</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span>
    
    <span class="k">def</span> <span class="nf">__zero_except</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">            A helper function to replace all but the ith column with zeroes, and </span>
<span class="sd">            return the result. (There is probably a cleaner way to do this.)</span>
<span class="sd">        """</span>
        <span class="n">X_copy</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">X_i</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span>
        <span class="n">X_copy</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">X_copy</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_i</span>
        <span class="k">return</span> <span class="n">X_copy</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Using-the-wrapper">Using the wrapper<a class="anchor-link" href="posts/building-a-utility-function-wrapper-for-scikit-learn-models/#Using-the-wrapper">¶</a>
</h3>
<p>Now we have our <code>ModelUtils</code> wrapper class, lets import <code>CustomModel</code> <a href="posts/accessing-jupyter-notebooks-programatically">as before</a> and put it to work.</p>
<p>As we instantiate the wrapper, we're specifying <code>test.pkl</code> in the current directory as the location to serialize the model.</p>
<p>If <code>serialize_path</code> is configured and valid, the pre-fitted model will be loaded from there, and the <code>predict</code> function will be immediately available. If configured but the file does not exist, <code>ModelUtils</code> will serialize to this location after fitting the model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ipynb.fs.defs.model</span> <span class="k">import</span> <span class="n">CustomModel</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ModelUtils</span><span class="p">(</span><span class="n">CustomModel</span><span class="p">(),</span> <span class="n">serialize_path</span><span class="o">=</span><span class="s1">'test.pkl'</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's load up the sample data again, and fit our model and then use it to create some predictions. Note the <code>Saved to test.pkl</code> output.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> 
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span> <span class="c1"># more reproducibility</span>

<span class="c1"># fit our model (as before)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># generate some predictions (as before)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span>  <span class="c1"># fun fact: the ; character suppresses notebook output</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Saved to test.pkl
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Model-and-feature-performance">Model and feature performance<a class="anchor-link" href="posts/building-a-utility-function-wrapper-for-scikit-learn-models/#Model-and-feature-performance">¶</a>
</h3>
<p>Finally, here are our two new functions.</p>
<p>First, let's score the performance of our model. This is using a metric called ROC AUC - we won't explain what that is in this post in any detail, but essentially it is a measure of how well the model can separate each class in <code>y</code>.</p>
<p>Then we will calculate relative feature importance for each of the 30 features in the sample dataset. Based on individual scoring performance, what this means is the the first feature is ~2.12x more powerful than the lowest performing feature, and the best feature is ~35.86x more powerful.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># score our model</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'AUC:'</span><span class="p">,</span> <span class="n">auc</span><span class="p">)</span>

<span class="c1"># calculate relative feature importance</span>
<span class="n">importance</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importance</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Top feature relative performance:'</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">importance</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importance</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>AUC: 0.989669421488
Top feature relative performance: 35.855513308
[  2.121673     8.6730038   34.85551331   2.06844106  25.14068441
  29.74904943  33.68060837  34.68441065  24.88973384  15.51711027
   4.66920152  18.64638783   4.85931559  34.7148289   16.36121673
  24.82129278  27.2851711   27.96958175  15.74904943  16.24334601   1.
  29.3269962   35.85551331  35.83269962  26.1634981   30.85931559
  33.77186312  35.12927757  27.24714829  23.27376426]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>👏</p>

</div>
</div>
</div>
    </div>
  </div>

    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/accessing-jupyter-notebooks-programatically/" class="u-url">Accessing Jupyter notebooks programatically</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Blair Hudson
            </span></p>
            <p class="dateline"><a href="posts/accessing-jupyter-notebooks-programatically/" rel="bookmark"><time class="published dt-published" datetime="2017-09-17T20:16:20+10:00" title="2017-09-17 20:16">2017-09-17 20:16</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In a previous post we <a href="posts/introduction-to-classification-using-logistic-regression-with-scikit-learn">created a simple classifier</a> using Scikit-Learn's <code>LogisticRegression</code>.</p>
<p>As we pieced together our model, we structured the code into a class called <code>CustomModel</code>, with two functions: <code>fit</code> and <code>predict</code>.</p>
<p>To start working programatically with the notebook created in that post, you will first need to install the <code>ipynb</code> package:</p>

<pre><code>pip install git+https://github.com/blairhudson/ipynb</code></pre>
<p>(Note: This is actually a fork of an <a href="https://github.com/ipython/ipynb">IPython repo</a>. Unfortunately the master has a bug with parsing tuple-based assignments (e.g. <code>X, y = ...</code>). A <a href="https://github.com/ipython/ipynb/pull/34">pull request</a> has been submitted.)</p>
<p>Now you're ready to go!</p>
<h3 id="Using-the-ipynb-package">Using the ipynb package<a class="anchor-link" href="posts/accessing-jupyter-notebooks-programatically/#Using-the-ipynb-package">¶</a>
</h3>
<p>To simplify things considerably, make sure that you have a copy of <a href="posts/introduction-to-classification-using-logistic-regression-with-scikit-learn/Introduction%20to%20Classification%20using%20Logistic%20Regression%20with%20Scikit-Learn.ipynb">the source</a> in the current working directory, and rename it to <code>model.ipynb</code>.</p>
<p>Now, thanks to the <code>ipynb</code> package you can access the <code>CustomModel</code> class like this:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ipynb.fs.defs.model</span> <span class="k">import</span> <span class="n">CustomModel</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To prove it, let's generate predictions on the same sample data:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> 
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span> <span class="c1"># more reproducibility</span>

<span class="c1"># load our model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CustomModel</span><span class="p">()</span>

<span class="c1"># fit our model</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># generate some predictions</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[6]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>array([  9.25168417e-01,   9.99922130e-01,   9.53635418e-01,
         9.88416588e-01,   9.97542577e-01,   9.95232506e-01,
         4.60659258e-02,   9.98390194e-01,   6.59002902e-10,
         2.76899836e-06,   8.30718694e-10,   9.63993586e-01,
         9.94157890e-01,   9.50980576e-01,   9.96974859e-01,
         6.97038792e-10,   9.99809391e-01,   9.96431765e-01,
         9.99363563e-01,   8.43800531e-06,   9.95502414e-01,
         7.77576547e-03,   1.12727716e-09,   3.40904102e-17,
         3.68627970e-09,   6.55649762e-01,   3.51723839e-03,
         9.97326888e-01,   9.98785233e-01,   9.97552026e-01,
         9.86350517e-01,   9.98844211e-01,   5.70842717e-04,
         9.87742427e-01,   9.19814189e-01,   9.78443649e-01,
         9.92882821e-01,   1.14676290e-02,   1.48817234e-01,
         9.98733024e-01,   4.13813658e-05,   9.93177003e-01,
         1.72319657e-10,   8.54534408e-01,   8.81187668e-01,
         9.97568264e-01,   9.98086681e-01,   8.32784885e-01,
         4.49929586e-11,   8.89087737e-01,   9.28259947e-01,
         9.91244116e-01,   9.94876558e-01,   1.51106510e-08,
         2.60668778e-01,   9.99597520e-01,   9.98940073e-01,
         9.99968817e-01,   9.91318570e-01,   8.29369844e-03,
         9.93238377e-01,   9.92431535e-01,   9.29775117e-01,
         9.99271713e-01,   9.96474598e-01,   2.41572863e-04,
         1.51376226e-11,   9.97330558e-01,   9.98831771e-01,
         4.79400697e-01,   9.99798779e-01,   3.57307727e-07,
         9.99656809e-01,   7.03641088e-01,   9.98247027e-01,
         9.96093354e-01,   9.99588791e-01,   2.58369708e-08,
         9.98136922e-01,   7.97865310e-03,   9.99065333e-01,
         9.98470351e-01,   9.94581260e-01,   9.29328694e-01,
         1.41996390e-02,   1.43214384e-04,   3.71155631e-05,
         4.45838811e-06,   9.13207438e-01,   8.56295696e-01,
         9.99467328e-01,   9.74324559e-01,   9.99328632e-01,
         2.91312374e-12,   1.00998256e-01,   9.86992421e-01,
         9.97149193e-01,   9.13815924e-01,   9.98807818e-01,
         9.84005486e-01,   3.17865443e-08,   2.30937811e-11,
         9.98036358e-01,   9.99532884e-01,   1.24075526e-03,
         9.98819765e-01,   9.99752279e-01,   8.53677349e-04,
         1.53192255e-01,   9.30832406e-01,   1.49723823e-05,
         5.28688983e-01,   1.48786146e-03,   9.92804571e-51,
         8.86447353e-01,   9.95516043e-01,   9.98554149e-01,
         1.75078944e-03,   9.99922978e-01,   4.67159833e-01,
         9.99825913e-01,   9.57716419e-01,   9.95069689e-01,
         9.98728887e-01,   7.49375338e-14,   9.92513330e-01,
         1.49918676e-02,   1.63977226e-02,   9.95785292e-01,
         9.56124754e-01,   3.53639065e-01,   9.96011137e-01,
         7.27728677e-33,   9.97779030e-01,   7.77872222e-02,
         9.90058068e-01,   9.80367925e-01,   2.92408222e-01,
         9.98164180e-01,   1.67926421e-01,   9.99996297e-01,
         6.35631576e-10,   1.06440027e-01])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Magic ✨</p>

</div>
</div>
</div>
    </div>
  </div>

    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/using-jupyter-notebooks-with-anaconda/" class="u-url">Using Jupyter notebooks with Anaconda</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Blair Hudson
            </span></p>
            <p class="dateline"><a href="posts/using-jupyter-notebooks-with-anaconda/" rel="bookmark"><time class="published dt-published" datetime="2017-09-16T20:08:20+10:00" title="2017-09-16 20:08">2017-09-16 20:08</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Jupyter is a popular data science environment, and Jupyter notebooks (such as <a href="posts/using-jupyter-notebooks-with-anaconda/Using%20Jupyter%20notebooks%20with%20Anaconda.ipynb">the notebook this post</a> was written with) are a great way to create and share great data science with inline documentation (using Markdown syntax).</p>
<p>Jupyter is capable of running kernels in many different programming languages, but in this post we're focussed just on Python.</p>
<h3 id="Installing-Anaconda">Installing Anaconda<a class="anchor-link" href="posts/using-jupyter-notebooks-with-anaconda/#Installing-Anaconda">¶</a>
</h3>
<ol>
<li>Download Anaconda for your operating system from the <a href="https://www.anaconda.com/download">Anaconda website</a>. For best compatibility with modern data science packages, I suggest Python 3.6 version or newer.</li>
<li>Run the downloaded installer and follow the prompts.</li>
</ol>
<h3 id="Launching-Jupyter">Launching Jupyter<a class="anchor-link" href="posts/using-jupyter-notebooks-with-anaconda/#Launching-Jupyter">¶</a>
</h3>
<ol>
<li>Run the following command to launch the Jupyter environment in your current directory:
 <code>jupyter notebook</code>
</li>
<li>By default, this will open the web interface in your default web browser, and by default at <a href="http://localhost:8888/">http://localhost:8888/</a>.</li>
<li>Now you can select an existing <code>.ipynb</code> file from the file navigator to open it, or create a new notebook.</li>
</ol>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"🚀"</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>🚀
</pre>
</div>
</div>

</div>
</div>

</div>
    </div>
  </div>

    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/" class="u-url">Optimising hyper-parameters efficiently with Scikit-Optimize</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Blair Hudson
            </span></p>
            <p class="dateline"><a href="posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/" rel="bookmark"><time class="published dt-published" datetime="2017-09-16T15:59:59+10:00" title="2017-09-16 15:59">2017-09-16 15:59</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One of the most well-known techniques for experimenting with various model configurations is <em>Grid Search</em>.</p>
<p>With grid search, you specify a discrete search space (a parameter grid) of all of the parameter values you would like to test. The search permutes through the grid, testing various combinations until all are exhausted. Basic a specified performance metric (e.g. error), you can select the best parameter combination for your model.</p>
<p>What's wrong with this?</p>
<p>If you have a large parameter grid, this doesn't work too well:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'param_a'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="s1">'param_b'</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="s1">'param_c'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">num_searches</span><span class="p">(</span><span class="n">param_grid</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>
    
<span class="n">num_searches</span><span class="p">(</span><span class="n">param_grid</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[1]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>27</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And maybe we want to search over four possible values instead for <code>param_a</code>, and add two more new parameters:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'param_a'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="s1">'param_b'</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="s1">'param_c'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="s1">'param_d'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"a"</span><span class="p">,</span> <span class="s2">"b"</span><span class="p">],</span>
    <span class="s1">'param_e'</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">num_searches</span><span class="p">(</span><span class="n">param_grid</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[2]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>216</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see from the first grid, there's already 27 combinations to try. Then this jumps to 216 for our larger grid. Depending on the complexity of the model and the amount of data to process, this can very easily become infeasible.</p>
<p>There are a few approaches to solving this, including:</p>
<ul>
<li>breaking down the search into multiple smaller steps (such as searching <code>param_a</code> and <code>param_b</code> first, with defaults for the others, then using the best values to search the remaining parameters - this can be tricky in practice)</li>
<li>searching the parameter space <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html">at random</a> (which has an additional benefit of discovering better parameter values when random samples are drawn frmo a continuous range)</li>
</ul>
<p>While Scikit-Learn doesn't provide many more options, some <a href="https://github.com/scikit-optimize/scikit-optimize/blob/master/AUTHORS.md">clever people</a> have developed a drop-in replacement for Scikit-Learn's <code>GridSearchCV</code> and <code>RandomizedSearchCV</code> called <code>BayesSearchCV</code> in a package called <em>Scikit-Optimize</em>.</p>
<p>Let's install Scikit-Optimize and implement <code>BayesSearchCV</code> with a simple example!</p>
<h3 id="Installing-Scikit-Optimize">Installing Scikit-Optimize<a class="anchor-link" href="posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/#Installing-Scikit-Optimize">¶</a>
</h3>
<p>Assuming you already have already <a href="posts/using-jupyter-notebooks-with-anaconda/">installed Anaconda and Jupyter</a>, you will need to do the following:</p>
<ul>
<li><code>pip install scikit-optimize</code></li>
</ul>
<p>If you have trouble installing, you may first need to run the following to install one of Scikit-Optmize's dependencies:</p>
<ul>
<li><code>pip install scikit-garden</code></li>
</ul>
<h3 id="Implementing-BayesSearchCV">Implementing BayesSearchCV<a class="anchor-link" href="posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/#Implementing-BayesSearchCV">¶</a>
</h3>
<p>Here's an example implementation using a sample dataset and <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">Logistic Regression</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">skopt</span> <span class="k">import</span> <span class="n">BayesSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># prep some sample data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>

<span class="c1"># we're using a logistic regression model</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># this is our parameter grid</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'solver'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'liblinear'</span><span class="p">,</span> <span class="s1">'saga'</span><span class="p">],</span>  
    <span class="s1">'penalty'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'l1'</span><span class="p">,</span><span class="s1">'l2'</span><span class="p">],</span>
    <span class="s1">'tol'</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="s1">'log-uniform'</span><span class="p">),</span>
    <span class="s1">'C'</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="s1">'log-uniform'</span><span class="p">),</span>
    <span class="s1">'fit_intercept'</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># set up our optimiser to find the best params in 30 searches</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">BayesSearchCV</span><span class="p">(</span>
    <span class="n">clf</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="p">,</span>
    <span class="n">n_iter</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>

<span class="n">opt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'Best params achieve a test score of'</span><span class="p">,</span> <span class="n">opt</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="s1">':'</span><span class="p">)</span>

<span class="n">opt</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Best params achieve a test score of 0.958041958042 :
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[4]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>{'C': 100.0,
 'fit_intercept': True,
 'penalty': 'l1',
 'solver': 'liblinear',
 'tol': 0.00094035472283658726}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By increasing the value of <code>n_iter</code>, you can continue the search to find better parameter combinations. You can also use the optimiser for prediction, by calling <code>.predict()</code> or <code>.predict_proba()</code> for probabilities, or extract and use the best one standalone:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">opt</span><span class="o">.</span><span class="n">best_estimator_</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[5]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=1234, solver='liblinear',
          tol=0.00094035472283658726, verbose=0, warm_start=False)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You may also find it useful to re-use the best parameters programatically to define an equivalent model:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">LogisticRegression</span><span class="p">(</span><span class="o">**</span><span class="n">opt</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[6]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear',
          tol=0.00094035472283658726, verbose=0, warm_start=False)</pre>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>

    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/creating-a-blog-with-jupyter-notebooks/" class="u-url">Creating a blog with Jupyter notebooks</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Blair Hudson
            </span></p>
            <p class="dateline"><a href="posts/creating-a-blog-with-jupyter-notebooks/" rel="bookmark"><time class="published dt-published" datetime="2017-09-13T21:29:29+10:00" title="2017-09-13 21:29">2017-09-13 21:29</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Assuming you already have already <a href="posts/using-jupyter-notebooks-with-anaconda/">installed Jupyter notebook</a>, you will need to do the following:</p>
<h3 id="Installing-and-configuring-a-Nikola-blog">Installing and configuring a Nikola blog<a class="anchor-link" href="posts/creating-a-blog-with-jupyter-notebooks/#Installing-and-configuring-a-Nikola-blog">¶</a>
</h3>
<ol>
<li>
<p>First you'll need to create a directory structure as follows:</p>

<pre><code> - /blog
 -- /posts
 -- /output</code></pre>
<ul>
<li>
<code>/blog</code> is the root directory for everything you'll be doing with your blog</li>
<li>
<code>/posts</code> is where you'll store your Jupyter notebooks</li>
<li>
<code>/output</code> will contain the code generated for your blog</li>
</ul>
</li>
<li>
<p>Run the following command to install Nikola (the static website generator which will do most of the heavy lifting)<sup>[1]</sup>:</p>
<p><code>pip install --upgrade "Nikola[extras]"</code></p>
</li>
<li>
<p>Change directory to your blog root:</p>
<p><code>cd blog</code></p>
</li>
<li>
<p>Start up Nikola, following the prompts to configure your new blog:</p>
<p><code>nikola init .</code></p>
</li>
<li>
<p>Open <code>/blog/conf.py</code> and change the <code>POSTS</code> and <code>PAGES</code> sections to include the lines as follows. This will allow Nikola to treat <code>.ipynb</code> files as blog posts.</p>

<pre><code> POSTS = (
     ("posts/*.rst", "posts", "post.tmpl"),
     ("posts/*.md", "posts", "post.tmpl"),
     ("posts/*.txt", "posts", "post.tmpl"),
     ("posts/*.html", "posts", "post.tmpl"),
     ("posts/*.ipynb", "posts", "post.tmpl"),
 )
 PAGES = (
     ("pages/*.rst", "pages", "page.tmpl"),
     ("pages/*.md", "pages", "page.tmpl"),
     ("pages/*.txt", "pages", "page.tmpl"),
     ("pages/*.html", "pages", "page.tmpl"),
     ("pages/*.ipynb", "pages", "page.tmpl"),
 )</code></pre>
</li>
<li>
<p>Write your blog post in Jupyter, saving the <code>.ipynb</code> file to <code>/posts</code>.</p>
</li>
<li>
<p>You will need to explicitly add the following metadata to your notebook (in the Jupyter menu, select <em>Edit &gt; Edit Notebook Metadata</em>). Change the metadata to match your post.<sup>[2]</sup></p>

<pre><code> "nikola": {
     "title": "Creating a blog with Jupyter notebooks",
     "slug": "creating-a-blog-with-jupyter-notebooks",
     "date": "2017-09-09 21:09:01 UTC+10:00"
 }</code></pre>
</li>
<li>
<p>Run <code>nikola build</code> each time you update your <code>/posts</code>, which will generate your site and store it in <code>/output</code>!</p>
</li>
<li>
<p>If you're going to be publishing your blog on Github (like me), you can push the content of <code>/output</code> to your website repo (<a href="https://github.com/blairhudson/blog">example</a>).</p>
</li>
</ol>
<h4 id="[1]Problems-installing-Nikola?">
<sup>[1]</sup>Problems installing Nikola?<a class="anchor-link" href="posts/creating-a-blog-with-jupyter-notebooks/#%5B1%5DProblems-installing-Nikola?">¶</a>
</h4>
<p>I ran into some issues installing Nikola on OS X with Anaconda. Specifically, <code>gcc</code> in Anaconda was the culprit. Resolution:</p>
<ul>
<li>
<code>conda remove gcc</code> to uninstall <code>gcc</code> provided by Anaconda</li>
</ul>
<p>This will default to the system <code>gcc</code>, which you can check by running <code>which gcc</code> (which should output <code>/usr/bin/gcc</code>).</p>
<p>If this still doesn't resolve the issue still, you may need to install a more up-to-date <code>gcc</code>:</p>
<ol>
<li>Install <a href="https://brew.sh">Homebrew</a>
</li>
<li>
<code>brew install gcc</code> (you may be prompted to install Developer Tools)</li>
<li><code>brew unlink gcc</code></li>
<li><code>brew link --overwrite gcc</code></li>
</ol>
<p><code>which gcc</code> should now show <code>/usr/local/Cellar/gcc/7.2.0</code>. 👍</p>
<h4 id="[2]Inferring-Nikola-post-metadata">
<sup>[2]</sup>Inferring Nikola post metadata<a class="anchor-link" href="posts/creating-a-blog-with-jupyter-notebooks/#%5B2%5DInferring-Nikola-post-metadata">¶</a>
</h4>
<p>Like me, you probably want as little as possible to come between your latest notebook hack and your awesome new blog.</p>
<p>Nikola parses Jupyter notebooks with a plugin, which with some modification we can have infer all of the Nikola post metadata automatically. For me, the plugin file was here (though it may differ for you):</p>
<p><code>~/anaconda/lib/python3.5/site-packages/nikola/plugins/compile/ipynb.py</code></p>
<p>To automagically infer the required metadata, you can replace the <code>read_metadata()</code> function in the file above with the following code:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">read_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">file_metadata_regexp</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">unslugify_titles</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">"""Read metadata directly from ipynb file.</span>

<span class="sd">    As ipynb file support arbitrary metadata as json, the metadata used by Nikola</span>
<span class="sd">    will be assume to be in the 'nikola' subfield.</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_req_missing_ipynb</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">lang</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lang</span> <span class="o">=</span> <span class="n">LocaleBorg</span><span class="p">()</span><span class="o">.</span><span class="n">current_lang</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">translated_source_path</span><span class="p">(</span><span class="n">lang</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">"utf8"</span><span class="p">)</span> <span class="k">as</span> <span class="n">in_file</span><span class="p">:</span>
        <span class="n">nb_json</span> <span class="o">=</span> <span class="n">nbformat</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">in_file</span><span class="p">,</span> <span class="n">current_nbformat</span><span class="p">)</span>
    <span class="c1"># Metadata might not exist in two-file posts or in hand-crafted</span>
    <span class="c1"># .ipynb files.</span>

    <span class="c1"># infer metadata</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">source</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">slug</span> <span class="o">=</span> <span class="n">title</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">' '</span><span class="p">,</span> <span class="s1">'-'</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">datetime</span> <span class="k">import</span> <span class="n">datetime</span>
    <span class="n">date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">fromtimestamp</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getctime</span><span class="p">(</span><span class="n">source</span><span class="p">))</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">'%Y-%m-</span><span class="si">%d</span><span class="s1"> %k:%M:%S'</span><span class="p">)</span>

    <span class="n">implicit</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'title'</span><span class="p">:</span><span class="n">title</span><span class="p">,</span> <span class="s1">'slug'</span><span class="p">:</span> <span class="n">slug</span><span class="p">,</span> <span class="s1">'date'</span><span class="p">:</span><span class="n">date</span><span class="p">}</span>
    <span class="n">explicit</span> <span class="o">=</span> <span class="n">nb_json</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'metadata'</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'nikola'</span><span class="p">,</span> <span class="p">{})</span>
    
    <span class="c1"># replace inference with explicit if available</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">implicit</span><span class="p">,</span> <span class="o">**</span><span class="n">explicit</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">metadata</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With this small modification, we instruct Nikola to infer the <code>title</code> and <code>slug</code> values based on the filename, and the <code>date</code> value based on the filesystem. Magical! ✨</p>
<p><strong>Update:</strong> The makers of Nikola have suggested some official methods for achieving this that are built right into the existing workflow:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">html</span>
&lt;blockquote class="twitter-tweet" data-conversation="none" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;Titles and slugs can be done via FILE_METADATA_REGEXP, and auto dates are prone to issues.&lt;br&gt;Better: import files with `nikola new_post -i`&lt;/p&gt;&amp;mdash; Nikola Generator (@GetNikola) &lt;a href="https://twitter.com/GetNikola/status/907570254611484672"&gt;September 12, 2017&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">Titles and slugs can be done via FILE_METADATA_REGEXP, and auto dates are prone to issues.<br>Better: import files with `nikola new_post -i`</p>— Nikola Generator (@GetNikola) <a href="https://twitter.com/GetNikola/status/907570254611484672">September 12, 2017</a>
</blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>

    </div>
    </article>
</div>





        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
        </script><!--End of body content--><footer id="footer">
            Contents © 2017 <a href="http://blairhudson.com">Blair Hudson</a> • Views are my own
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-99092313-1', 'auto');
    ga('send', 'pageview');
</script>
</body>
</html>
